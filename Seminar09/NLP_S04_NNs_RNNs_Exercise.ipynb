{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e363c3d",
   "metadata": {},
   "source": [
    "# NLP Seminar 4: Text classification with neural networks (MLPs, RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3f8d4",
   "metadata": {},
   "source": [
    "In this seminar, we cover the implementation of \"all-in-one\" text classification neural networks using the `tensorflow` package. The resulting models include both a neural text embedding and a neural classifier in a single pipeline.\n",
    "\n",
    "The focus is here on two different types of architectures: multi-layer perceptrons (MLPs) and (bidirectional) recurrent neural netrowks (RNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64633a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ffd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6638bd1",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0da84b",
   "metadata": {},
   "source": [
    "We again use the Simpsons script dataset. As a toy example, the classification goal is to determine which character is speaking given a textual dialogue line.\n",
    "\n",
    "Data source: https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfe53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = pd.read_csv(\"data/simpsons_script_lines.csv\",\n",
    "                       usecols=[\"raw_character_text\", \"raw_location_text\", \"spoken_words\", \"normalized_text\"],\n",
    "                       dtype={'raw_character_text':'string', 'raw_location_text':'string',\n",
    "                              'spoken_words':'string', 'normalized_text':'string'})\n",
    "simpsons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad41f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b721d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = simpsons.dropna().drop_duplicates().reset_index(drop=True)\n",
    "simpsons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539589e1",
   "metadata": {},
   "source": [
    "We restrict the data to the 10 most frequent characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "simpsons['raw_character_text'].value_counts(dropna=False)[:n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbebc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_characters = simpsons['raw_character_text'].value_counts(dropna=False)[:n_classes].index.to_list()\n",
    "\n",
    "simpsons_main = simpsons.query(\"`raw_character_text` in @main_characters\")\n",
    "simpsons_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a15d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = simpsons_main[\"normalized_text\"].to_numpy()\n",
    "y = simpsons_main[\"raw_character_text\"].to_numpy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_int = np.array([np.where(np.array(main_characters)==char)[0].item() for char in y])\n",
    "y_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f37455",
   "metadata": {},
   "source": [
    "Train-validation data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y_int, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03d898",
   "metadata": {},
   "source": [
    "## TextVectorization and Embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736fb0b",
   "metadata": {},
   "source": [
    "In a `Sequantial` tensorflow model, the [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer is used to transform raw text into bag of words.\n",
    "For efficiency reasons, the bag-of-words representations are encoded as integers instead of one-hot-vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 100\n",
    "sequence_length = 10\n",
    "vectorize_layer = layers.TextVectorization(max_tokens=max_vocab, standardize='lower_and_strip_punctuation',\n",
    "                                           output_mode='int', output_sequence_length=sequence_length)\n",
    "vectorize_layer.adapt(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c66b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97df7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba234ba",
   "metadata": {},
   "source": [
    "Then, the [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer is usually used to transform the bag-of-words integers to (trainable) vectorial embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = layers.Embedding(input_dim=len(vectorize_layer.get_vocabulary()), output_dim=8,\n",
    "                            embeddings_initializer='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90527f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ff94d",
   "metadata": {},
   "source": [
    "## MLP network architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbade44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "max_vocab = 10000\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b93d90",
   "metadata": {},
   "source": [
    "The `TextVectorization` layer needs to be \"adapted\" (i.e. needs to construct the vocabulary) before beeing added in a `Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b2ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(max_tokens=??, standardize='lower_and_strip_punctuation',#or custom\n",
    "                                           output_mode='int', output_sequence_length=??)\n",
    "vectorize_layer.adapt(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorize_layer.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08d21b",
   "metadata": {},
   "source": [
    "We construct a toy MLP example on top of the embedding layers, using the `Sequential` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=50\n",
    "\n",
    "MLP_model = Sequential([\n",
    "    ??,\n",
    "    layers.Embedding(input_dim=??, output_dim=??,\n",
    "                     embeddings_initializer='uniform'),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "], name=\"MLP_model\")\n",
    "\n",
    "MLP_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb6228",
   "metadata": {},
   "source": [
    "We assign the `Adam` optimizer and the `SparseCategoricalCrossentropy` loss to the model for training.\n",
    "We then train the model on the training data and log the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "history_MLP = MLP_model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                            batch_size=128, epochs=epochs)\n",
    "\n",
    "MLP_model.save_weights(\"./checkpoints/MLP_10/MLP_10\")\n",
    "#MLP_model.save(\"./checkpoints/MLP_10_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60086caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.load_weights(\"./checkpoints/MLP_10/MLP_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a62b40",
   "metadata": {},
   "source": [
    "We evaluate the model on the validation data, and inspect the training `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f483d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "?, ? = MLP_model.evaluate(??, ??)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MLP_dict = history_MLP.history\n",
    "history_MLP_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history_dict):\n",
    "    acc = history_dict['accuracy']\n",
    "    val_acc = history_dict['val_accuracy']\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, 'b--', label='Training')\n",
    "    plt.plot(epochs, val_loss, 'r-', label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cross-Entropy Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, acc, 'b--', label='Training')\n",
    "    plt.plot(epochs, val_acc, 'r-', label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_training(history_MLP_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba868c",
   "metadata": {},
   "source": [
    "We predict the expected character for a few new line examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6846f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"Homer, come here!\",\n",
    "            \"Science is amazing.\",\n",
    "            \"Duh\"]\n",
    "\n",
    "MLP_model.predict(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0bd5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(tf_model, data, class_names=main_characters):\n",
    "    \"\"\"Returns the string model predictions (class name of the largest network output activation).\"\"\"\n",
    "    return np.array(class_names)[tf_model.predict(data).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc64ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(??, ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5de45a",
   "metadata": {},
   "source": [
    "A few tutorials for more details:\n",
    "- https://www.tensorflow.org/text/guide/word_embeddings\n",
    "- https://www.tensorflow.org/tutorials/keras/text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37f31b",
   "metadata": {},
   "source": [
    "##  Recurrent network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61290f",
   "metadata": {},
   "source": [
    "We now construct a few recurrent architectures on top of the embedding layers. Those allow to capture sequential dependencies in the input text sequences through latent variables, and also dealing with varying text sequence lengths.\n",
    "\n",
    "The three main type of recurrent layers implemented in `tensorflow` are: `SimpleRNN`, `LSTM` and `GRU`.\n",
    "We here use the LSTM layers, but one can in principle substitute it for any of the others.\n",
    "\n",
    "As the meaning of a word can sometimes depend not only on the context preceding it, but also succeeding it, \"Bidirectional\" recurrent architectures are often used in some NLP tasks. The `Bidirectional` layer wrapper allows to implement this easily in a `Sequential` model\n",
    "\n",
    "A few tutorials to go further:\n",
    "- https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "- https://www.tensorflow.org/guide/keras/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 10000\n",
    "vectorize_layer = layers.TextVectorization(max_tokens=max_vocab, standardize='lower_and_strip_punctuation',\n",
    "                                           output_mode='int', output_sequence_length=None)\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ffa8d",
   "metadata": {},
   "source": [
    "Here are a few example of LSTM-based architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=50\n",
    "\n",
    "# Using masking with 'mask_zero=True' to handle the variable sequence lengths in subsequent layers.\n",
    "\n",
    "LSTM_model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    layers.Embedding(input_dim=len(vectorize_layer.get_vocabulary()), output_dim=embedding_dim,\n",
    "                     embeddings_initializer='uniform', mask_zero=True),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "], name=\"LSTM_model\")\n",
    "\n",
    "BLSTM_model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    layers.Embedding(input_dim=len(vectorize_layer.get_vocabulary()), output_dim=embedding_dim,\n",
    "                     embeddings_initializer='uniform', mask_zero=True),\n",
    "    layers.Bidirectional(layers.LSTM(64), merge_mode='concat'),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "], name=\"BLSTM_model\")\n",
    "\n",
    "BLSTM2_model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    layers.Embedding(input_dim=len(vectorize_layer.get_vocabulary()), output_dim=embedding_dim,\n",
    "                     embeddings_initializer='uniform', mask_zero=True),\n",
    "    layers.Bidirectional(layers.LSTM(64,  return_sequences=True), merge_mode='concat'),\n",
    "    layers.Bidirectional(layers.LSTM(32), merge_mode='concat'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "], name=\"BLSTM2_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68dbdb7",
   "metadata": {},
   "source": [
    "(Using masking with `'mask_zero=True'` in the embedding to handle the variable sequence lengths in subsequent layers: https://www.tensorflow.org/guide/keras/masking_and_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535319da",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLSTM_model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "epochs = 10#100\n",
    "history_lstm = BLSTM_model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                               batch_size=128, epochs=epochs)\n",
    "\n",
    "BLSTM_model.save_weights(\"./checkpoints/BLSTM_10/BLSTM_10\")\n",
    "#BLSTM_model.save(\"./checkpoints/BLSTM_10_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab85061",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLSTM_model.load_weights(\"./checkpoints/BLSTM_10/BLSTM_10\")\n",
    "history_lstm_dict = history_lstm.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = BLSTM_model.evaluate(X_valid, y_valid)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n",
    "\n",
    "plot_training(history_lstm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbcae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLSTM_model.predict(examples)\n",
    "predict_class(BLSTM_model, examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ac066",
   "metadata": {},
   "source": [
    "## Using pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a003277",
   "metadata": {},
   "source": [
    "Instead of randomly initializing the `Embedding` layer, one can give pretrained word vectors to it.\n",
    "We here use pretrained GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3aa342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as gensim_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99418275",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "glv_embd = gensim_api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 10000\n",
    "vectorize_layer = layers.TextVectorization(max_tokens=max_vocab, standardize='lower_and_strip_punctuation',\n",
    "                                           output_mode='int', output_sequence_length=None)\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorize_layer.get_vocabulary(include_special_tokens=True)\n",
    "embedding_matrix = np.random.uniform(-0.05, 0.05, (len(vocab), embedding_dim))\n",
    "\n",
    "oov_words = list()\n",
    "for i, w in enumerate(vocab):\n",
    "    try:\n",
    "        embedding_matrix[i,] = glv_embd.get_vector(w, norm=True)\n",
    "    except:\n",
    "        embedding_matrix[i,] = embedding_matrix[1,]\n",
    "        oov_words += [w]\n",
    "\n",
    "#print(oov_words)\n",
    "print(len(oov_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = layers.Embedding(input_dim=len(vectorize_layer.get_vocabulary()), output_dim=embedding_dim,\n",
    "                            embeddings_initializer=keras.initializers.Constant(??),\n",
    "                            mask_zero=True, trainable=False, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ee088",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glv_embd.get_vector(\"finally\", norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f854f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(glv_embd.get_vector(\"finally\", norm=True) != embedder(vectorize_layer(X_train[[7]])).numpy()[0,0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained embeddings:\n",
    "BLSTM_Glv_model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    layers.Embedding(input_dim=len(vectorize_layer.get_vocabulary()), output_dim=embedding_dim,\n",
    "                     embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                     mask_zero=True, trainable=False, name='embedding'),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "], name=\"BLSTM_Glv_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c207ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLSTM_Glv_model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                        loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "epochs_f = 5\n",
    "epochs_t = epochs_f + 10\n",
    "\n",
    "#Probably not ideal, but as an idea of what one can do:\n",
    "\n",
    "history_glv_f = BLSTM_Glv_model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                                    batch_size=128, epochs=epochs_f)\n",
    "BLSTM_Glv_model.save_weights(\"./checkpoints/Glv_5/Glv_5\")\n",
    "\n",
    "BLSTM_Glv_model.layers[1].trainable = True\n",
    "\n",
    "history_glv_l = BLSTM_Glv_model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                                    batch_size=128, epochs=epochs_t, initial_epoch=epochs_f)\n",
    "BLSTM_Glv_model.save_weights(\"./checkpoints/Glv_15/Glv_15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ae68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLSTM_Glv_model.load_weights(\"./checkpoints/Glv_15/Glv_15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab52be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = BLSTM_Glv_model.evaluate(X_valid, y_valid)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e4e7d",
   "metadata": {},
   "source": [
    "# Exercise: Fine tuning\n",
    "\n",
    "Try to achieve the best validation accuracy by fine-tuning the hyperparameters of any method above.\n",
    "\n",
    "The following two `callbacks` examples could be useful during training:\n",
    "\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=1)\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath = \"checkpoint/metwork_epoch={epoch:02d}-val_accuracy={val_accuracy:.2f}.h5\")\n",
    "    callbacks=[early_stopping_cb, checkpoint_cb]\n",
    "\n",
    "To go further, `keras_tuner` can also be useful: https://www.tensorflow.org/tutorials/keras/keras_tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08ee8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd97c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fcdf8b7",
   "metadata": {},
   "source": [
    "#### draft..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.dump(history_MLP.history, open(\"./checkpoints/history/MLP_10.txt\", 'w'))\n",
    "history_MLP_dict = json.load(open(\"./checkpoints/history/MLP_10.txt\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7073f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.dump(history_lstm.history, open(\"./checkpoints/history/BLSTM_10.txt\", 'w'))\n",
    "history_lstm_dict = json.load(open(\"./checkpoints/history/BLSTM_10.txt\", 'r'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
