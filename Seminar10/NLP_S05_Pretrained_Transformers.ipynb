{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8268bc47",
   "metadata": {},
   "source": [
    "# NLP Seminar 5: Pretrained Transformers and Transfer-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48722a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efa03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1655fb0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16eb59",
   "metadata": {},
   "source": [
    "Transformers can be implemented from scratch in both tensorflow and pytorch\n",
    "(e.g. https://www.tensorflow.org/text/tutorials/transformer).\n",
    "The multi-headed attention layers used in transformers are also implemented as a Keras layer in tensorflow ([Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention), [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)).\n",
    "However, constructing or reproducing meaningful transformer architectures from scratch, even with these building blocks, can still remain challenging. This is especially true for some of the more complex MLP tasks, combining encoder and decoder transformers.\n",
    "Furthermore, transformers have really proved their state-of-the art efficiency for NLP taskes when trained on huge corpora of data. In particular, for many specific tasks, transfer learning is used to leverage the dynamic semantic information already aquired by pre-trained models.\n",
    "\n",
    "Although transfer-learning using pre-trained transformers such as BERT is possible with tensorflow (e.g. [classify_text_with_bert](https://www.tensorflow.org/text/tutorials/classify_text_with_bert), [fine_tune_bert](https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert)) this practical will instead introduce the `HuggingFace` transformer library, as it\n",
    "- has a lot of pretrained stae-of-the-art transformer models for various tasks,\n",
    "- has a very high-level user-friendly interface,\n",
    "- is compatibile with both tensorflow and pytorch,\n",
    "- is used by many universities, research labs and companies.\n",
    "\n",
    "If needed, see the official tutorials to go further:\n",
    "- https://huggingface.co/learn/nlp-course/chapter1/1\n",
    "- https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25780041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec86e9",
   "metadata": {},
   "source": [
    "## Pre-trained pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608bf97",
   "metadata": {},
   "source": [
    "The `pipeline` allows loading pre-trained models with a very easy interface, for a wide range of different tasks from the `HuggingFace` database. Almost all main open-source pretrained transformer references (BERT, GPT, ...) are available.\n",
    "\n",
    "- Selected transformer architectures: https://huggingface.co/docs/transformers/index\n",
    "- comunity checkpoints: https://huggingface.co/models\n",
    "\n",
    "Here are a few examples of pretrained transformer models (i.e. checkpoints) for some NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ba0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc1d6e",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cf0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipe = pipeline(\"sentiment-analysis\", # or \"text-classification\"\n",
    "                     model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb7887f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998775720596313}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pipe(\"I love this product.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6eb31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9999\n",
      "label: NEGATIVE, with score: 0.9998\n"
     ]
    }
   ],
   "source": [
    "results = sent_pipe([\"I love this product.\",\n",
    "                    \"I hate this product.\"])\n",
    "\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e41bb",
   "metadata": {},
   "source": [
    "#### \"Zero-shot\" classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b11e53",
   "metadata": {},
   "source": [
    "Using natural language inference models to predict *entailment* between each sequence-label premise/hypothesis pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0244c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsc_pipe = pipeline('zero-shot-classification',\n",
    "                    model=\"facebook/bart-large-mnli\", revision=\"c626438\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33dfde41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I like trains.',\n",
       " 'labels': ['vehicles', 'animals', 'politics'],\n",
       " 'scores': [0.9911500811576843, 0.004934574011713266, 0.003915328532457352]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zsc_pipe(\"I like trains.\", [\"politics\",\"vehicles\",\"animals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbfb58",
   "metadata": {},
   "source": [
    "#### Dynamic word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6c9d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dwe_pipe = pipeline(\"feature-extraction\", model=\"bert-base-cased\") # e.g. \"bert-base-cased\" \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14f31052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8597,  0.1216, -0.0761,  ..., -0.1833,  0.1758,  0.0822],\n",
       "         [ 0.8721, -0.3453,  0.5372,  ..., -0.1697,  0.0987,  0.1806],\n",
       "         [ 0.6673, -0.0972, -0.5464,  ...,  0.5048, -0.4832,  0.1718],\n",
       "         [ 0.4819, -0.0781,  0.1035,  ...,  0.0943, -0.3238,  0.1974],\n",
       "         [ 0.8397, -0.1973, -0.0363,  ..., -0.0951,  0.3160, -0.0663],\n",
       "         [ 1.6680,  0.1131, -0.2623,  ..., -0.2753,  0.5270, -0.1521]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dwe_pipe(\"I like trains.\", return_tensors=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba10a4",
   "metadata": {},
   "source": [
    "This outputs the last transformer block output. Some other embedding approaches exist, like averaging or concatenating the activations of several of the transformer's layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873fee0",
   "metadata": {},
   "source": [
    "#### Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf77c74",
   "metadata": {},
   "source": [
    "With causal language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a002eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3050884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this NLP seminar about transformers, we will learn all about a basic transformation, especially involving a bit of information about the model that describes the form, the transformation, and the parameters for the transformation. However, this class is intended as an'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"In this NLP seminar about transformers, we will learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7560a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this NLP seminar about transformers, we will learn more about transformation and how you can improve your NLP with the tools we have. We'},\n",
       " {'generated_text': 'In this NLP seminar about transformers, we will learn about all of the transformers in the program that will use the transforms in the application.'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"In this NLP seminar about transformers, we will learn\",\n",
    "          max_length=30,\n",
    "          num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4561cc6",
   "metadata": {},
   "source": [
    "#### Mask filling\n",
    "This language model task is part of how BERT architectures are often pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b48250e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82977257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19378963112831116,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This seminar will teach you all about mathematical models.'},\n",
       " {'score': 0.04502846300601959,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This seminar will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"This seminar will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cb8e0",
   "metadata": {},
   "source": [
    "#### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "565f3924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\pascheo\\Anaconda3\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad3160a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9992693,\n",
       "  'word': 'Olivier',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.97970057,\n",
       "  'word': 'University of Geneva',\n",
       "  'start': 37,\n",
       "  'end': 57},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9608657,\n",
       "  'word': 'Plainpalais',\n",
       "  'start': 63,\n",
       "  'end': 74}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Olivier and I work at the University of Geneva near Plainpalais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129be83d",
   "metadata": {},
   "source": [
    "#### Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75629f",
   "metadata": {},
   "source": [
    "Using extractive encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d12b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bcd89ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7924373149871826,\n",
       " 'start': 37,\n",
       " 'end': 57,\n",
       " 'answer': 'University of Geneva'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=\"Where do I work?\",\n",
    "                  context=\"My name is Olivier and I work at the University of Geneva near Plainpalais. I have a lot of work at the office this week.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234865c5",
   "metadata": {},
   "source": [
    "#### Document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858893d",
   "metadata": {},
   "source": [
    "Using encoder-decoder tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02d90576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5eace5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them . The technology can then extract information and insights contained in the documents as well as categorize and organize the documents themselves . Challenges in natural language processing frequently involve speech recognition, natural-language .'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    # Wikipedia page on NLP:\n",
    "    \"\"\"\n",
    "    Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science,\n",
    "    and artificial intelligence concerned with the interactions between computers and human language,\n",
    "    in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "    The goal is a computer capable of \"understanding\" the contents of documents, including the contextual\n",
    "    nuances of the language within them. The technology can then accurately extract information and insights\n",
    "    contained in the documents as well as categorize and organize the documents themselves.\n",
    "    \n",
    "    Challenges in natural language processing frequently involve speech recognition, natural-language\n",
    "    understanding, and natural-language generation.\n",
    "    \n",
    "    Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article\n",
    "    titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a\n",
    "    criterion of intelligence, though at the time that was not articulated as a problem separate from\n",
    "    artificial intelligence. The proposed test includes a task that involves the automated interpretation\n",
    "    and generation of natural language.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29568d5",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276730d4",
   "metadata": {},
   "source": [
    "Using encoder-decoder tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e77fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pascheo\\Anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e699010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This seminar is exceptionally given on Tuesday afternoon.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"Ce séminaire est exceptionnellement donné le mardi après-midi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fed091",
   "metadata": {},
   "source": [
    "## What constitues a pipeline?\n",
    "\n",
    "Example for a classification pipeline.\n",
    "\n",
    "https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31af0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "sent_pipe = pipeline(\"sentiment-analysis\", model=pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6aef063",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = [\"I love this amazing Transformers introduction seminar.\",\n",
    "        \"I hate debugging my code so much!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7ed7854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998568296432495},\n",
       " {'label': 'NEGATIVE', 'score': 0.9962984919548035}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pipe(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525e26c",
   "metadata": {},
   "source": [
    "### Step 1. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae42545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98f5e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5ba81d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[  101,  1045,  2293,  2023,  6429, 19081,  4955, 18014,  1012,\n",
      "          102,     0,     0],\n",
      "       [  101,  1045,  5223,  2139,  8569, 12588,  2026,  3642,  2061,\n",
      "         2172,   999,   102]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary BoW indexes for the tokenized text corpus:\n",
    "inputs = tokenizer(corp, padding=True, truncation=True, return_tensors=\"np\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c401a7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101,  1045,  2293,  2023,  6429, 19081,  4955, 18014,  1012,\n",
       "          102,     0,     0],\n",
       "       [  101,  1045,  5223,  2139,  8569, 12588,  2026,  3642,  2061,\n",
       "         2172,   999,   102]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For comparison, the pipeline tokenizer is the same:\n",
    "sent_pipe.tokenizer(corp, padding=True, truncation=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5b594fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'this', 'amazing', 'transformers', 'introduction', 'seminar', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'hate', 'de', '##bu', '##gging', 'my', 'code', 'so', 'much', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#Textual version of the tokens:\n",
    "for doc in corp:\n",
    "    print(tokenizer.tokenize(doc, add_special_tokens=True)) # sub-word / wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8310c05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i love this amazing transformers introduction seminar. [SEP] [PAD] [PAD]'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 1045, 2293, 2023, 6429, 19081, 4955, 18014, 1012, 102, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72c406fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i hate debugging my code so much! [SEP]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 1045, 5223, 2139, 8569, 12588, 2026, 3642, 2061, 2172, 999, 102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456505b",
   "metadata": {},
   "source": [
    "### Step 2.1. Transformer model\n",
    "\n",
    "https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40618403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4ecc3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['classifier', 'pre_classifier', 'dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c58be445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,362,880\n",
      "Trainable params: 66,362,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "150157a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(2, 12, 768), dtype=float32, numpy=\n",
       "array([[[ 0.74096173,  0.09954479,  0.17740408, ...,  0.37085024,\n",
       "          1.0475554 , -0.53418267],\n",
       "        [ 0.9534427 ,  0.17849974,  0.07076181, ...,  0.3553438 ,\n",
       "          1.1498722 , -0.3271943 ],\n",
       "        [ 1.1049628 ,  0.2552293 ,  0.33767316, ...,  0.3171587 ,\n",
       "          1.0195711 , -0.37932056],\n",
       "        ...,\n",
       "        [ 1.1075138 ,  0.08023867,  0.6883422 , ...,  0.6212788 ,\n",
       "          0.59678227, -0.8048649 ],\n",
       "        [ 0.58498317,  0.11724679,  0.09631445, ...,  0.5941567 ,\n",
       "          1.0606877 , -0.28705198],\n",
       "        [ 0.57371145,  0.08199537,  0.07770054, ...,  0.43690923,\n",
       "          1.0691313 , -0.36507556]],\n",
       "\n",
       "       [[-0.12865362,  0.6125047 , -0.43362248, ...,  0.02990204,\n",
       "         -0.37186334,  0.16652946],\n",
       "        [-0.08211757,  0.91229707, -0.15405867, ..., -0.02132722,\n",
       "         -0.25966606,  0.25903928],\n",
       "        [-0.05406427,  0.6817771 , -0.06585205, ...,  0.07498453,\n",
       "         -0.32770878,  0.12720117],\n",
       "        ...,\n",
       "        [-0.39177468,  0.50210387, -0.42975897, ...,  0.04451011,\n",
       "         -0.8621884 , -0.03974984],\n",
       "        [ 0.13117838,  0.8358262 , -0.50160366, ...,  0.14485833,\n",
       "         -0.3103754 ,  0.04740486],\n",
       "        [ 0.21348709,  0.5011952 , -0.22045547, ..., -0.03712624,\n",
       "         -0.51862264, -0.09755814]]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8821b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 768)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce0df1",
   "metadata": {},
   "source": [
    "### Step 2.2. Transformer model with classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac6e3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5529e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_38']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classif_model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac20a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classif_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33a8b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = classif_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b7b5c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-4.2869983,  4.5641413],\n",
       "       [ 3.06056  , -2.5347545]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8e358a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-4.2869983,  4.5641413],\n",
       "       [ 3.06056  , -2.5347545]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea35fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(outputs2.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e20c4",
   "metadata": {},
   "source": [
    "### Step 3. Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "285d374f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.431980e-04, 9.998568e-01],\n",
       "       [9.962985e-01, 3.701479e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = tf.keras.activations.softmax(outputs2.logits, axis=-1)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba8da367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted class:\n",
    "probabilities.numpy().argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d51eca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9998568, 0.9962985], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction certainty:\n",
    "probabilities.numpy().max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f4d1cb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class labels corresponding to the integer incoding:\n",
    "classif_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf41a662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998568296432495},\n",
       " {'label': 'NEGATIVE', 'score': 0.9962984919548035}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For comparison, the pipeline output:\n",
    "sent_pipe(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd67700",
   "metadata": {},
   "source": [
    "For different tasks, thare might be additionnal preprocessing and feature extraction steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be7e01",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "298bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "tf_save_directory = \"./checkpoints/tf_save_pretrained\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "classif_model.save_pretrained(tf_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d1b218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./checkpoints/tf_save_pretrained were not used when initializing TFDistilBertForSequenceClassification: ['dropout_38']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./checkpoints/tf_save_pretrained and are newly initialized: ['dropout_58']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load\n",
    "classif_model = TFAutoModelForSequenceClassification.from_pretrained(\"./checkpoints/tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e0587e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./checkpoints/tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fdfcf",
   "metadata": {},
   "source": [
    "## Transfer learning with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618b778",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f996876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = pd.read_csv(\"../Seminar08/data/simpsons_script_lines.csv\",\n",
    "                       usecols=[\"raw_character_text\", \"raw_location_text\", \"spoken_words\", \"normalized_text\"],\n",
    "                       dtype={'raw_character_text':'string', 'raw_location_text':'string',\n",
    "                              'spoken_words':'string', 'normalized_text':'string'})\n",
    "simpsons = simpsons.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "n_classes = 10\n",
    "main_characters = simpsons['raw_character_text'].value_counts(dropna=False)[:n_classes].index.to_list()\n",
    "simpsons_main = simpsons.query(\"`raw_character_text` in @main_characters\")\n",
    "\n",
    "X = simpsons_main[\"normalized_text\"].to_numpy()\n",
    "y = simpsons_main[\"raw_character_text\"].to_numpy()\n",
    "y_int = np.array([np.where(np.array(main_characters)==char)[0].item() for char in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2c6f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y_int, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00668b08",
   "metadata": {},
   "source": [
    "#### Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "607dd1f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfefe035387f4cc1b16bac9e0c99479b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/527M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pretrained_name2 = \"bert-base-cased\" # e.g. \"bert-base-cased\" or \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name2)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_name2, num_labels=n_classes)#?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d3afb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  7690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,317,962\n",
      "Trainable params: 108,317,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead0f6e",
   "metadata": {},
   "source": [
    "To freeze the pretrained transformer weights, and only train the classification head, we can set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "934802ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec55f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  7690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,317,962\n",
      "Trainable params: 7,690\n",
      "Non-trainable params: 108,310,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a635333",
   "metadata": {},
   "source": [
    "Allowing the transformer weights to be modified by leaving `model.layers[0].trainable = True` can significantly improve performance of the downstream task, but will take significantly longer to train. Furthermore, more care needs to be taken when selecting the training hyperparameters (low initial learning rate, learning rate decay, not too many epochs), to prevent [Catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference), and loosing pretraining information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b4149286",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok = dict(tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors=\"tf\"))\n",
    "X_valid_tok = dict(tokenizer(X_valid.tolist(), padding=True, truncation=True, return_tensors=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e2355cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=3e-5),\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fit is very long without a GPU or a cloud service\n",
    "epochs = 5\n",
    "history_ft = model.fit(X_train_tok, y_train, validation_data=(X_valid_tok, y_valid),\n",
    "                       batch_size=16, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc9db1",
   "metadata": {},
   "source": [
    "For larger dataset sizes, one can perform more efficient training using HuggingFace's `Datasets`, that can allow for smarter parallel memory allocation from disk: https://huggingface.co/docs/datasets/index.\n",
    "\n",
    "HuggingFace's `Datasets` can then also interract with the Keras API (e.g. the `model.fit()` method), for example through `model.prepare_tf_dataset()` or `Dataset.to_tf_dataset()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
