{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8268bc47",
   "metadata": {},
   "source": [
    "# NLP Seminar 5: Pretrained Transformers and Transfer-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48722a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1655fb0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16eb59",
   "metadata": {},
   "source": [
    "Transformers can be implemented from scratch in both tensorflow and pytorch\n",
    "(e.g. https://www.tensorflow.org/text/tutorials/transformer).\n",
    "The multi-headed attention layers used in transformers are also implemented as a Keras layer in tensorflow ([Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention), [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)).\n",
    "However, constructing or reproducing meaningful transformer architectures from scratch, even with these building blocks, can still remain challenging. This is especially true for some of the more complex MLP tasks, combining encoder and decoder transformers.\n",
    "Furthermore, transformers have really proved their state-of-the art efficiency for NLP taskes when trained on huge corpora of data. In particular, for many specific tasks, transfer learning is used to leverage the dynamic semantic information already aquired by pre-trained models.\n",
    "\n",
    "Although transfer-learning using pre-trained transformers such as BERT is possible with tensorflow (e.g. [classify_text_with_bert](https://www.tensorflow.org/text/tutorials/classify_text_with_bert), [fine_tune_bert](https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert)) this practical will instead introduce the `HuggingFace` transformer library, as it has\n",
    "- a lot of pretrained stae-of-the-art transformer models for various tasks,\n",
    "- a very high-level user-friendly interface,\n",
    "- compatibility with both tensorflow and pytorch.\n",
    "\n",
    "If needed, see the official tutorials to go further:\n",
    "- https://huggingface.co/learn/nlp-course/chapter1/1\n",
    "- https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25780041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec86e9",
   "metadata": {},
   "source": [
    "## Pre-trained pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608bf97",
   "metadata": {},
   "source": [
    "The `pipeline` allows loading pre-trained models with a very easy interface, for a wide range of different tasks from the `HuggingFace` database. Almost all main open-source pretrained transformer references (BERT, GPT, ...) are available.\n",
    "\n",
    "- Selected transformer architectures: https://huggingface.co/docs/transformers/index\n",
    "- comunity checkpoints: https://huggingface.co/models\n",
    "\n",
    "Here are a few examples of pretrained transformer models (i.e. checkpoints) for some NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc1d6e",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipe = pipeline(\"sentiment-analysis\", # or \"text-classification\"\n",
    "                     model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipe(\"This restaurant is awesome.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6eb31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sent_pipe([??, ??])\n",
    "\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e41bb",
   "metadata": {},
   "source": [
    "#### \"Zero-shot\" classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b11e53",
   "metadata": {},
   "source": [
    "Using natural language inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsc_pipe = pipeline('zero-shot-classification',\n",
    "                    model=\"facebook/bart-large-mnli\", revision=\"c626438\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsc_pipe(\"I like trains.\", [\"politics\",\"vehicles\",\"animals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbfb58",
   "metadata": {},
   "source": [
    "#### Dynamic word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dwe_pipe = pipeline(\"feature-extraction\", model=\"bert-base-cased\") # e.g. \"bert-base-cased\" \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f31052",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dwe_pipe(\"I like trains.\", return_tensors=True)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba10a4",
   "metadata": {},
   "source": [
    "This outputs the last transformer block output. Some other embedding approaches exist, like averaging or concatenating the activations of several of the transformer's layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873fee0",
   "metadata": {},
   "source": [
    "#### Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf77c74",
   "metadata": {},
   "source": [
    "With causal language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "generator(\"In this NLP seminar about transformers, we will learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7560a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"In this NLP seminar about transformers, we will learn\",\n",
    "          max_length=30,\n",
    "          num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4561cc6",
   "metadata": {},
   "source": [
    "#### Mask filling\n",
    "This language model taks is part of how BERT architectures are often pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82977257",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "unmasker(\"This seminar will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cb8e0",
   "metadata": {},
   "source": [
    "#### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3160a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "ner(\"My name is Olivier and I work at the University of Geneva near Plainpalais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129be83d",
   "metadata": {},
   "source": [
    "#### Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75629f",
   "metadata": {},
   "source": [
    "Using extractive encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "question_answerer(question=\"Where do I work?\",\n",
    "                  context=\"My name is Olivier and I work at the University of Geneva near Plainpalais. I have a lot of work this week.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234865c5",
   "metadata": {},
   "source": [
    "#### Document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858893d",
   "metadata": {},
   "source": [
    "Using encoder-decoder tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eace5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "summarizer(\n",
    "    # Wikipedia page on NLP:\n",
    "    \"\"\"\n",
    "    Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science,\n",
    "    and artificial intelligence concerned with the interactions between computers and human language,\n",
    "    in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "    The goal is a computer capable of \"understanding\" the contents of documents, including the contextual\n",
    "    nuances of the language within them. The technology can then accurately extract information and insights\n",
    "    contained in the documents as well as categorize and organize the documents themselves.\n",
    "    \n",
    "    Challenges in natural language processing frequently involve speech recognition, natural-language\n",
    "    understanding, and natural-language generation.\n",
    "    \n",
    "    Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article\n",
    "    titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a\n",
    "    criterion of intelligence, though at the time that was not articulated as a problem separate from\n",
    "    artificial intelligence. The proposed test includes a task that involves the automated interpretation\n",
    "    and generation of natural language.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29568d5",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e699010",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "\n",
    "translator(\"Ce séminaire est exceptionnellement donné le mardi après-midi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fed091",
   "metadata": {},
   "source": [
    "## What constitues a pipeline?\n",
    "\n",
    "Example for a classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "sent_pipe = pipeline(\"sentiment-analysis\", model=pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aef063",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = [\"I love this amazing Transformers introduction seminar.\",\n",
    "        \"I hate debugging my code so much!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipe(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525e26c",
   "metadata": {},
   "source": [
    "### Step 1. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(corp, padding=True, truncation=True, return_tensors=\"np\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipe.tokenizer(corp, padding=True, truncation=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b594fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corp:\n",
    "    print(tokenizer.tokenize(doc, add_special_tokens=True)) # sub-word / wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([101, 1045, 2293, 2023, 6429, 19081, 4955, 18014, 1012, 102, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c406fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([101, 1045, 5223, 2139, 8569, 12588, 2026, 3642, 2061, 2172, 999, 102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456505b",
   "metadata": {},
   "source": [
    "### Step 2.1. Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40618403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58be445",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModel.from_pretrained(pretrained_name)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8821b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce0df1",
   "metadata": {},
   "source": [
    "### Step 2.2. Transformer model with classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_name)\n",
    "classif_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = classif_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs2.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e20c4",
   "metadata": {},
   "source": [
    "### Step 3. Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = tf.keras.activations.softmax(outputs2.logits, axis=-1)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8da367",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.numpy().argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.numpy().max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf41a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pipe(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd67700",
   "metadata": {},
   "source": [
    "For different tasks, thare might be additionnal preprocessing and feature extraction steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be7e01",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "tf_save_directory = \"./checkpoints/tf_save_pretrained\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "classif_model.save_pretrained(tf_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "classif_model = TFAutoModelForSequenceClassification.from_pretrained(\"./checkpoints/tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fdfcf",
   "metadata": {},
   "source": [
    "## Transfer learning with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618b778",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = pd.read_csv(\"../Seminar08/data/simpsons_script_lines.csv\",\n",
    "                       usecols=[\"raw_character_text\", \"raw_location_text\", \"spoken_words\", \"normalized_text\"],\n",
    "                       dtype={'raw_character_text':'string', 'raw_location_text':'string',\n",
    "                              'spoken_words':'string', 'normalized_text':'string'})\n",
    "simpsons = simpsons.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "n_classes = 10\n",
    "main_characters = simpsons['raw_character_text'].value_counts(dropna=False)[:n_classes].index.to_list()\n",
    "simpsons_main = simpsons.query(\"`raw_character_text` in @main_characters\")\n",
    "\n",
    "X = simpsons_main[\"normalized_text\"].to_numpy()\n",
    "y = simpsons_main[\"raw_character_text\"].to_numpy()\n",
    "y_int = np.array([np.where(np.array(main_characters)==char)[0].item() for char in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y_int, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00668b08",
   "metadata": {},
   "source": [
    "#### Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dd1f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pretrained_name2 = \"bert-base-cased\"#\"distilbert-base-uncased\" \"bert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name2)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_name2, num_labels=n_classes)#?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3afb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead0f6e",
   "metadata": {},
   "source": [
    "To freeze the pretrained transformer weights, and only train the classification head, we can set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934802ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a635333",
   "metadata": {},
   "source": [
    "Allowing the transformer weights to be modified by leaving `model.layers[0].trainable = True` can significantly improve performance of the downstream task, but will take significantly longer to train. Furthermore, more care needs to be taken when selecting the training hyperparameters (low initial learning rate, learning rate decay, not too many epochs), to prevent [Catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference), and loosing pretraining information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0be73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def tokenize_dataset(dataset):\n",
    "#    return tokenizer(dataset[\"text\"])\n",
    "#\n",
    "#dataset = dataset.map(tokenize_dataset)\n",
    "#tf_dataset = model.prepare_tf_dataset(dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)\n",
    "#\n",
    "#model.compile(optimizer=optimizers.Adam(learning_rate=3e-5))\n",
    "#model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4149286",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok = dict(tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors=\"np\"))\n",
    "X_valid_tok = dict(tokenizer(X_valid.tolist(), padding=True, truncation=True, return_tensors=\"np\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2355cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=3e-5),\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fit is very long without a GPU or a cloud service\n",
    "epochs = 5\n",
    "#history_ft = model.fit(X_train_tok, y_train, validation_data=(X_valid_tok, y_valid),\n",
    "#                       batch_size=16, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc9db1",
   "metadata": {},
   "source": [
    "For larger dataset sizes, one can perform more efficient training using HuggingFace's `Datasets`, that can allow for smarter parallel memory allocation from disk: https://huggingface.co/docs/datasets/index.\n",
    "\n",
    "HuggingFace's `Datasets` can then also interract with the Keras API (e.g. the `model.fit()` method), for example through `model.prepare_tf_dataset()` or `Dataset.to_tf_dataset()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
