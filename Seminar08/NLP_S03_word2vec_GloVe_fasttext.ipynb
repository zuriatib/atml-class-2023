{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e363c3d",
   "metadata": {},
   "source": [
    "# NLP Seminar 3: static word embeddings (word2vec, fasttext, GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3f8d4",
   "metadata": {},
   "source": [
    "In this seminar, we will use the `gensim` package, as it has unifying easy-to-use implementations and pretrained word2vec, fasttext, and GloVe models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc88c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64633a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6638bd1",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfe53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = pd.read_csv(\"data/simpsons_script_lines.csv\",\n",
    "                       usecols=[\"raw_character_text\", \"raw_location_text\", \"spoken_words\", \"normalized_text\"],\n",
    "                       dtype={'raw_character_text':'string', 'raw_location_text':'string',\n",
    "                              'spoken_words':'string', 'normalized_text':'string'})\n",
    "simpsons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad41f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b721d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = simpsons.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tok = simpsons['normalized_text'].str.split().to_list()\n",
    "corpus_tok[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e26241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know the Simpsons tv show, \n",
    "# you can e.g. use the wikipedia subset corpus instead,\n",
    "# (and try different words when evaluating the vectors and similarities in next sections):\n",
    "\n",
    "#import gensim.downloader as gensim_api\n",
    "#gensim_api.info('text8')['description']\n",
    "#corpus_tok = gensim_api.load('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b207b86",
   "metadata": {},
   "source": [
    "## Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174a551",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "phrases = Phrases(corpus_tok, min_count=30)\n",
    "phraser = Phraser(phrases)\n",
    "del(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7400ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "phraser[[\"homer\", \"simpson\", \"eats\", \"chocolate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_phrased = phraser[corpus_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de72f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_phrased[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a99c2",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61299b3",
   "metadata": {},
   "source": [
    "Word2vec has two sub-methods for training the word embeddings: continuous bag of words (CBOW) and skip-gram.\n",
    "In both cases, a shallow neural network is trained to predict either\n",
    "\n",
    "- a word given a context (CBOW), or\n",
    "- a context of a given a word (skip-gram).\n",
    "\n",
    "The context is defined as the other surrounding words in a given window. The word embedding vectors are then obained from the two trained weight matrices for each word in the vocabulary.\n",
    "\n",
    "\n",
    "Official website: https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Original papers: http://arxiv.org/abs/1301.3781 and http://arxiv.org/abs/1310.4546"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fab24b",
   "metadata": {},
   "source": [
    "### Training word2vec on the Simpson scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba32471",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s = Word2Vec(corpus_phrased, vector_size=150, window=3, min_count=2, sg=0, negative=5, ns_exponent=0.75,\n",
    "                 alpha=0.025, min_alpha=0.0001, workers=cores-1, epochs=30)\n",
    "#1st line: Method's hyperparameters\n",
    "#2nd line: Optimization (gradient descent) hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf201943",
   "metadata": {},
   "source": [
    "Can also be done in separate steps:\n",
    "\n",
    "    w2v_s = Word2Vec(vector_size=150, window=3, min_count=2, sg=0, negative=5, ns_exponent=0.75,\n",
    "                     alpha=0.025, min_alpha=0.0001, workers=cores-1)\n",
    "    w2v_s.build_vocab(sentences, progress_per=10000)\n",
    "    w2v_s.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0366ec2",
   "metadata": {},
   "source": [
    "Word embedding vectors can then be obtained from the trained model, for each word in the training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_vector = w2v_s.wv.get_vector(\"homer\", norm=True) # Father of the Simpsons\n",
    "homer_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6ccba",
   "metadata": {},
   "source": [
    "For a given word or vector, one can query the other most similar word vectors, in terms of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fcd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.wv.most_similar(\"homer\") # Marge is Homer's wife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.wv.most_similar(homer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.wv.most_similar(\"homer_simpson\") # name bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67437c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.wv.most_similar(\"bart\") # Bart is the son, Lisa his sister and Milhouse his best friend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d2dfa",
   "metadata": {},
   "source": [
    "One can also compute the cosine similarity between two word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.wv.similarity('bart', 'lisa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.wv.similarity('bart', 'bart')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca51aad",
   "metadata": {},
   "source": [
    "Odd-one-out identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.wv.doesnt_match(['homer', 'patty', 'selma']) # Patty and Selma are Marge's twin sisters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc0f5e",
   "metadata": {},
   "source": [
    "Word analogies: how well do embeddings vectors capture intuitive semantic and syntactic analogy questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" Homer - man + woman = ? \" - i.e. \" man:Homer :: woman:? \"\n",
    "w2v_s.wv.most_similar(positive=[\"homer\", \"woman\"], negative=[\"man\"], topn=3) # Marge is Homer's wife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55155190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" woman - Marge + Homer = ? \" - i.e. \" Marge:Homer :: woman:? \" \n",
    "w2v_s.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f30f4",
   "metadata": {},
   "source": [
    "### Sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ca96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document2vec(tokens, embedding_wv, phraser=None, normalize=True):\n",
    "    \"\"\"Returns the embedding of a sentence or document as the mean of its tokens/words embeddings.\"\"\"\n",
    "    if phraser:\n",
    "        tokens = phraser[tokens]\n",
    "    sent_mean = np.array([embedding_wv.get_vector(tok, norm=normalize) for tok in tokens]).mean(axis=0)\n",
    "    return sent_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77703262",
   "metadata": {},
   "outputs": [],
   "source": [
    "document2vec([\"bart\", \"is\", \"grounded\"], w2v_s.wv, phraser=phraser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae21d85",
   "metadata": {},
   "source": [
    "### Pretrained word2vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07e270",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim-data#models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea08609",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as gensim_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret = gensim_api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or from downloaded source (e.g. https://code.google.com/archive/p/word2vec/):\n",
    "#v2w_pret = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret.most_similar(positive=[\"eat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fa1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret.similarity(\"eat\", 'consume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caefd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret.doesnt_match([\"eat\", 'dance', 'drink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab33091",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectcalc = v2w_pret.get_vector(\"king\", norm=True) - v2w_pret.get_vector(\"man\", norm=True) + v2w_pret.get_vector(\"woman\", norm=True)\n",
    "v2w_pret.most_similar(vectcalc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72e10b",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e09bf",
   "metadata": {},
   "source": [
    "Fastext is a static word embedding, that is very similar to word2vec. As a main difference, it uses character-level ngram vectors together with the word vectors.\n",
    "\n",
    "Advantages:\n",
    "+ Out of training vocabulary embeddings are obtainable.\n",
    "+ Better representation for rare words (that are semantically similar to others).\n",
    "+ Tends to perform better for syntactic tasks.\n",
    "+ Is more useful in morphologically rich languages (such as German, Arabic and Russian) compared to English (German example: 'table tennis' -> 'Tischtennis'), but it heavily depends on the task.\n",
    "+ might work better for small datasets.\n",
    "+ The \"official\" implementation is quite efficient, and allows training the embedding and classifier at once (see the official `fasttest` package documentation https://fasttext.cc/docs/en/python-module.html).\n",
    "\n",
    "Disatvantages:\n",
    "- Can overfit more easily, and is a bit harder to fine tune with the additionnal character ngram hyperparameters.\n",
    "- Tends to perform more poorly for semantic tasks.\n",
    "- May tend to privilege too much the morphologically close synonyms compared to other semantically closer synomyms.\n",
    "- Can be heavier to train.\n",
    "\n",
    "However, the differences between fasttext and word2vec thend to decrease as the size of the training corpus increases.\n",
    "\n",
    "Official website: https://fasttext.cc/\n",
    "\n",
    "Original paper: https://arxiv.org/abs/1607.04606"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2a656",
   "metadata": {},
   "source": [
    "### Training fasttext on the Simpson scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_s = FastText(corpus_phrased, vector_size=150, window=3, min_count=5, sg=0, negative=5, ns_exponent=0.75,\n",
    "                 min_n=3, max_n=6, #Additional fasttest hyperparameters\n",
    "                 alpha=0.025, min_alpha=0.0001, workers=cores-1, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ce61e",
   "metadata": {},
   "source": [
    "Can also be performed in separate steps:\n",
    "\n",
    "    fst_s = FastText(vector_size=150, window=3, min_count=5, sg=0, negative=5, ns_exponent=0.75,\n",
    "                     min_n = 1, max_n = 4,\n",
    "                     alpha=0.025, min_alpha=0.0001, workers=cores-1)\n",
    "    fst_s.build_vocab(corpus_phrased)\n",
    "    print(len(fst_s.wv.vocab.keys()))\n",
    "    fst_s.train(sentences, total_examples = fst_s.corpus_count, epochs=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1661375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"unige\" in fst_s.wv.index_to_key, \"unige\" in w2v_s.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(w2v_s.wv.get_vector(\"unige\", norm=True))\n",
    "except:\n",
    "    print(\"KeyError: the given token is not not present in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff45ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(fst_s.wv.get_vector(\"unige\", norm=True))\n",
    "except:\n",
    "    print(\"KeyError: the given token is not not present in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_s.wv.most_similar(\"homer\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_s.wv.most_similar(\"marge\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1181a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v_s.wv.most_similar(\"marge\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_s.wv.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc77d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_s.wv.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394fa992",
   "metadata": {},
   "source": [
    "### Pretrained fasttext vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_pret = gensim_api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or from downloaded source (e.g. https://fasttext.cc/docs/en/english-vectors.html):\n",
    "# fst_pret = FastText.load_fasttext_format('fasttest_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_pret.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c92413",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_pret.most_similar(\"consume\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2w_pret.most_similar(\"consume\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9f974",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442cfa50",
   "metadata": {},
   "source": [
    "Contrary to word2vec and fasttext, GloVe doesn't use skipgram or CBOW networks. GloVe relies on word-context co-occurrence matrix factorization to obtain the embedded word vectors.\n",
    "\n",
    "- GloVe can be longer to train on larger corpora, compared to word2vec.\n",
    "- It has fewer hyperparameters, so it's much easier to tune, but then cannot be fine tuned for a specific task.\n",
    "- word2vec and fasttext are in comparison much more sensitive to the coices of hyperparameters, and results can thus vary much more.\n",
    "\n",
    "Official website: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Original paper: https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9f4d3",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glv_pret = gensim_api.load(\"glove-wiki-gigaword-200\")\n",
    "\n",
    "# Are already available in gensim:\n",
    "#'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300',\n",
    "#'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294cb5f",
   "metadata": {},
   "source": [
    "From downloaded source (e.g. https://nlp.stanford.edu/projects/glove/), one can do:\n",
    "\n",
    "    from gensim.test.utils import datapath, get_tmpfile\n",
    "    from gensim.models import KeyedVectors\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove_file = datapath('DOWNLOADED_GLOVE_VECTORS.txt')\n",
    "    tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bac8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "glv_pret.most_similar(positive=[\"better\", \"fast\"], negative=[\"good\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glv_pret.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d253f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glv_pret.most_similar(\"consume\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a4491",
   "metadata": {},
   "source": [
    "### Remark: training GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8416f5",
   "metadata": {},
   "source": [
    "Training GloVe vectors is not possible with gensim. If interested, one can use the [official GloVe code](https://nlp.stanford.edu/projects/glove/) (command line interface).\n",
    "\n",
    "For a python interface, see for example the (\"toy implementation\") [`glove_python`](https://github.com/maciejkula/glove-python) pachage\n",
    "\n",
    "    !pip install glove_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d4a4b",
   "metadata": {},
   "source": [
    "### See also other vector embeddings..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1217d0",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim-data#models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13734dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_api.info('conceptnet-numberbatch-17-06-300')['description']\n",
    "#conceptnet = gensim_api.load(\"conceptnet-numberbatch-17-06-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d347",
   "metadata": {},
   "source": [
    "# Saving gensim models and word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9a923",
   "metadata": {},
   "source": [
    "One can save either the entire model (if further training is expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.save('word2vec_simpson_model')\n",
    "w2v_s = Word2Vec.load('word2vec_simpson_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40a19a",
   "metadata": {},
   "source": [
    "Or only the word vectors (the `KeyedVectors`-type attribute) if the vecors are final. They are much more memory-efficient to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64dadde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v_s.wv.save('word2vec_simpson_word_vectors')\n",
    "w2v_s_wv = KeyedVectors.load('word2vec_simpson_word_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347d1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9671a4f9",
   "metadata": {},
   "source": [
    "# Exercise: ML classification using advanced static embeddings\n",
    "\n",
    "Compare the performance of the logistic regression classifier on the 20newsgroup dataset using word2vect, GloVe or fasttext to the performance achieved in the previous seminar using TF-IDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
