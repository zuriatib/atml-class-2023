{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65559326e76d2b7841244e1dd6ea448671411b95"
   },
   "source": [
    "# NLP Seminar 1 - N-grams Language Models (N-grams LM)\n",
    "\n",
    "In this first NLLP seminar, we will focus on n-gram language models. They are a classical approach to sentence modelling, and text autocompletion.\n",
    "\n",
    "We will use the `nltk` (natural language toolkit) python package. \n",
    "If you want to learn more about this popular module, refer to the [official website](https://www.nltk.org/) ([API reference](https://www.nltk.org/api/nltk.html), [installation guide](https://www.nltk.org/install.html)).\n",
    "\n",
    "In particular, the `nltk.lm` submodule provides optimized implementations of classical n-grams language models such as the maximum likelihood estimator (MLE) and its smoothing variants (Laplace, Lidstone, ...).\n",
    "\n",
    "To illustrate the ngram approach, we will apply it on the Trump Tweets dataset, and try to generate new tweets!\n",
    "\n",
    "Before that, we will begin with the basics, by understanding how to preprocess the text data into tokens and ngrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b213fb0dd5534ce82d6f1c716e9695ba5cf9758"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First download some nltk resources\n",
    "# (By default '!pip install nltk' does not actually download every resource in the module,\n",
    "# as for example some language models are heavy.)\n",
    "# The following command should download every resource needed for this practical:\n",
    "nltk.download('popular', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we first consider the dummy corpus `text` with two documents/sequences of tokens. The tokens are here simple letters, but we can think of them as representing words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c1716261478af5e75bba7799bfefc10bd1ea4ea"
   },
   "outputs": [],
   "source": [
    "text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "800f7632b6a834d36b95211a912b390fcb9dc11b"
   },
   "source": [
    "If we want to train a bigram model, we need to turn this text into n-grams. We can use the `bigrams` and `ngrams` functions from NLTK as helpers, to turn the each token list document into an ngram list (for ex. with $n=2$ and $n=3$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5a1c775bab7cf9c67656d4349d8bfca02a80738"
   },
   "outputs": [],
   "source": [
    "list(bigrams(?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0219988318e39dd0576913b8087af8b35cbdab2f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(ngrams(??))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3a92b25dc25f3ae86fd265880b7f410e981c670"
   },
   "source": [
    "Notice how \"b\" occurs both as the first and second member of different bigrams but \"a\" and \"c\" don't? \n",
    "\n",
    "It would be nice to indicate to the model how often sentences start with \"a\" and end with \"c\" for example, when we will count those ngrams later-on.\n",
    "\n",
    "\n",
    "A standard way to deal with this is to add special \"padding\" symbols to the document/sequence before splitting it into ngrams. Fortunately, NLTK also has a `pad_sequence` function for that. We use `\"<s>\"` and `\"</s>\"` by convention in `nltk` to pad before and after the sequence, respectively.\n",
    "\n",
    "Lets add the relevent paddings and construct the bigrams and 3-grams for the first text sequence. Note the `n` argument, that tells the function we need padding for `n`-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ec58c94f58429c9160b4496c939f211f88ade54"
   },
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae38644238aa46ac100381c0de046972090b093b"
   },
   "outputs": [],
   "source": [
    "padded_seq = list(pad_sequence(??))\n",
    "list(ngrams(padded_seq, n=?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd572a57371a716471d156144e7ebc136072de01"
   },
   "outputs": [],
   "source": [
    "padded_seq = list(pad_sequence(??))\n",
    "list(ngrams(padded_seq, n=?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4d80e5707a2efa7ea5cee601d07bca23bc0ae87"
   },
   "source": [
    "Passing all these parameters every time can be tedious and in most cases one uses the same defaults anyway.\n",
    "\n",
    "Thus the `nltk.lm` module provides a convenience function that has all these arguments already set while the other arguments remain the same as for `pad_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb64c3c2a205c5fc5e1fce6f63c3e9038f0d8c4a"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb64c3c2a205c5fc5e1fce6f63c3e9038f0d8c4a"
   },
   "outputs": [],
   "source": [
    "list(pad_both_ends(?, n=?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d2581fae73d38d623a77b5d523c957df2dc8478"
   },
   "source": [
    "Combining the two parts discussed so far we get the following preparation steps for one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1168917cc340d400f6dba9b2703561785481d8e4"
   },
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2778a5e5b8d95bc395ba41af491818140c7f680e"
   },
   "source": [
    "For versatility and conditional probabilities, the `nltk.lm` n-gram models usually need everygrams of order n. For bigrams, they are trained using unigrams (single words) as well as bigrams. For 3-grams, they usually rely on unigrams, bigrams and 3-grams. And so on... \n",
    "NLTK once again helpfully provides a function called `everygrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfdc9effca718a7881f1fdc4562cdfb29164b96c"
   },
   "outputs": [],
   "source": [
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfdc9effca718a7881f1fdc4562cdfb29164b96c"
   },
   "outputs": [],
   "source": [
    "#with n=2:\n",
    "padded_bigrams = list(pad_both_ends(??))\n",
    "list(everygrams(padded_bigrams, max_len=?)) #train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d74b0ba683b733d5a83756361a905625a841e68d"
   },
   "source": [
    "We are almost ready to start counting ngrams, just one more step left.\n",
    "\n",
    "During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model, to efficiently perform the counting.\n",
    "\n",
    "To create this vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "310c937ea7cc847a26f6c3d08c7169e8cd6b354a"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "310c937ea7cc847a26f6c3d08c7169e8cd6b354a"
   },
   "outputs": [],
   "source": [
    "list(flatten(pad_both_ends(sent, n=2) for sent in text)) #vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bab7d656c474d945669e6a86bf4beec3a44189e"
   },
   "source": [
    "In most cases we want to use the same text as the source for both vocabulary and ngram counts.\n",
    "\n",
    "Now that we understand what this means for our preprocessing, we can simply import the `padded_everygram_pipeline` function that does exactly everything above for us for the whole corpus, in a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "147ccdd961ac97492ee9f0eadf3dfce7325c6790"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "147ccdd961ac97492ee9f0eadf3dfce7325c6790"
   },
   "outputs": [],
   "source": [
    "?? = padded_everygram_pipeline(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1b15504f2fb4487deec19de1c5ca786e49a42d6"
   },
   "source": [
    "So as to avoid re-creating the text in memory, both `train` and `vocab` are lazy iterators. They are evaluated on demand at training time.\n",
    "\n",
    "For the sake of understanding the output of `padded_everygram_pipeline`, we \"materialize\" the lazy iterators by casting them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa18a8a4a700926c30488c94cf519d8900de8ddf"
   },
   "outputs": [],
   "source": [
    "training_ngrams, padded_sequences = padded_everygram_pipeline(??)\n",
    "\n",
    "print('==== n-gram data (n=1,2) for each sequence in \"text\": ====')\n",
    "for ngramlize_sent in training_ngrams:\n",
    "    print(list(ngramlize_sent))\n",
    "    print()\n",
    "print('==== Vocabulary data: ====')\n",
    "print(list(padded_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4638a871a58694a77473a590728ebf25047cd6eb"
   },
   "source": [
    "# Tokenizing real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4f13ed7359c87e397229104c1bcf18eb20603ad"
   },
   "source": [
    "Lets try some text generation with Donald Trump tweets!\n",
    "\n",
    "\n",
    "**Dataset source:** https://www.kaggle.com/kingburrito666/better-donald-trump-tweets#Donald-Tweets!.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import, inspect and preprocess the text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "caf5fea33f84e06c3cf613136ec33f80e774fc98"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Trump_tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet_Text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet_Text'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facultative preprocessing and text wrangling ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then tokenize the text corpus (split around words and punctuation). `nltk.word_tokenize` is the recommended tokenizer in `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a15d9d1a754ca357ae79e8390069b08b05320458"
   },
   "outputs": [],
   "source": [
    "trump_corpus = list(df['Tweet_Text'].apply(??))\n",
    "print(trump_corpus[0])\n",
    "print(trump_corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32c5ad89e3e61b143e940d50a0d0ee602dadfe3f"
   },
   "source": [
    "## Training an N-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff1ff506e3a14df8283cb93b0a45f86862d3e3c7"
   },
   "source": [
    "Having prepared our data we are ready to start training a model. As a simple example, let us train a Maximum Likelihood Estimator (MLE).\n",
    "\n",
    "We first prepare the itterators for the everygrams and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8724b8724cc52b685205047bd1fda649545a6a24"
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = ?\n",
    "train_data, padded_seqs = padded_everygram_pipeline(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to specify the highest ngram order to instantiate the MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2635458b0f3fed618bfae6a0705ccb11555bfb0b"
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(?) # Lets train a 3-grams model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69cdd0e5babaf059a7b9bc87dc0a6e261cef6cb2"
   },
   "source": [
    "Initializing the MLE model, creates an empty vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93b95f89f020585dabfbcd6c41273c823a91e882"
   },
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb567e332a0089c86e388a8c93f32d1a31737a29"
   },
   "source": [
    "... which gets filled as we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a31ecc7f6df30a42df9ed6a79039b50f734299cf"
   },
   "outputs": [],
   "source": [
    "model.fit(?, ?)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fd7e68c2cdacbe1901fd3b8c05c440706d19100"
   },
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74622c58a2df2ac08c8b0c79644d1f87f4014f44"
   },
   "source": [
    "The vocabulary helps us handle words that have not occurred during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5670e34f9d1950f6a1515d57db18092d39484f2"
   },
   "outputs": [],
   "source": [
    "print(model.vocab.lookup(trump_corpus[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfec70768e8976073dda05f702f3f210a33dc1c3"
   },
   "outputs": [],
   "source": [
    "# If we lookup the vocab on unseen sentences not from the training data, \n",
    "# it automatically replace words not in the vocabulary with `<UNK>`.\n",
    "print(model.vocab.lookup('Busy day government erer .'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `padded_everygram_pipeline` returns itterators (that can only be used once), it might be a good idea to have the full pipeline in a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ngram_language_model(order, train_corpus_tokens, LM_Class=nltk.lm.MLE, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    :param order: integer stting the maximum order of the n-grams.\n",
    "    :param train_corpus_tokens: list of tokenized text sequences.\n",
    "    :param LM_Class: a language model as a nltk.lm.LanguageModel sub-class.\n",
    "    additional arguments are passed to `LM_Class`.\n",
    "    \"\"\"\n",
    "    ???\n",
    "    ???\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_model = fit_ngram_language_model(order=?, train_corpus_tokens=?, LM_Class=?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af5a0851a2f8707ea2e172681342ed3ecd872328"
   },
   "source": [
    "## Using the N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d098fd4686bcdc0ab9aa72dbbd4b62a17ee2c332"
   },
   "source": [
    "When it comes to ngram models the training boils down to counting up the ngrams from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfc60d61539269298390044c0d3415bfd28e2b1e"
   },
   "outputs": [],
   "source": [
    "print(mle_model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9b8cac68de80aee9c7cf7b7f2faf2b199ad27cc"
   },
   "source": [
    "This provides a convenient interface to access counts for unigrams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90f4611580c41118747a42cfac4c9d729f02523f"
   },
   "outputs": [],
   "source": [
    "mle_model.counts['America'] # i.e. Count('America')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_model.counts['Trump'] # i.e. Count('Trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccdf1bf83ba16f0a43f04eb96ca224a968cc81d2"
   },
   "source": [
    "...and bigrams for the phrase bit \"I will\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ba55a83d7c052d1f14923c4046a5f6c86d72c8c"
   },
   "outputs": [],
   "source": [
    "mle_model.counts[['I']][\"will\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88bceda0a01e9f2642dd30a6b341c24a606720b9"
   },
   "source": [
    "... and trigrams for the phrase bit \"will never forget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "813e30e1fb153002cfb020d430def2510bf43bbc"
   },
   "outputs": [],
   "source": [
    "mle_model.counts[('will', \"never\")][\"forget\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c4338d351cfc3cd785673ad9581ecab161b24c6"
   },
   "source": [
    "And so on. However, the real purpose of training a language model is to have it score how probable words are in certain contexts.\n",
    "\n",
    "This being MLE, the model returns the item's relative frequency as its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "051ef5a06c004a8e8ddb6168ca318bc8c0c9abf4"
   },
   "outputs": [],
   "source": [
    "mle_model.score('America') # P('America')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_model.score('Trump') # P('Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab3d48919b8f074342e624a1da441621829a4dbd"
   },
   "outputs": [],
   "source": [
    "mle_model.score('will', ('I',))  # P('will'|'I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_model.score(??) # P('forget'|'will never')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "946dde75e8d5b8d8878271359b0d91926abeb199"
   },
   "source": [
    "Items that are not seen during training are mapped to a specific vocabulary \"unknown label\" token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44038e8cd8d078b734a6b1a803d0795517e2fa82",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mle_model.score(\"<UNK>\"))\n",
    "print(mle_model.score(\"<UNK>\") == mle_model.score(\"erer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2f170a0bbca4b33b5eb1a55a55f0e1b1b18a634"
   },
   "outputs": [],
   "source": [
    "mle_model.score(\"<UNK>\") == mle_model.score(\"vava\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a9ad0be90c72ece6c2a1c00314ea60f41a00f81"
   },
   "source": [
    "To avoid underflow when working with many small score values it makes sense to take their logarithm. \n",
    "\n",
    "For convenience this can be done with the `logscore` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20ef81b7e161df3b9ab27236c33559713b3077ce"
   },
   "outputs": [],
   "source": [
    "mle_model.logscore('forget', ('will', 'never'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18f9e0a8d0aba302532b8a84f342d1bf4d3e202a"
   },
   "source": [
    "## Generation using N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f893159fe093aef484c07b37d922de4ac5727834"
   },
   "source": [
    "One cool feature of ngram models is that they can be used to generate text. The `nltk.lm.model` classes have a `.generate()` method to sample sequentially from the extimated (conditional) propabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98989ec4bae592fc98332e759daf9e42bac4213e"
   },
   "outputs": [],
   "source": [
    "print(mle_model.generate(??))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b8d07eaf3afae978131573ae127fa0ec3f2e50d"
   },
   "source": [
    "We can do some cleaning and detokenization in a function to make the generated tokens mor human-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73d0d5e0029e64876100e0f2e368b4835a99efcd"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "def tweet_detokenizer(token_list: list[str]) -> str:\n",
    "    TbDetok = TreebankWordDetokenizer()\n",
    "    tb_string = TbDetok.detokenize(token_list)\n",
    "    detokenized_tweet = tb_string.replace(' .','.').replace('@ ', '@')\n",
    "    return detokenized_tweet\n",
    "\n",
    "def generate_tweet(model, max_words, text_seed=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param max_words: Max no. of words to generate.\n",
    "    :param text_seed: Generation can be conditioned on preceding context tokens.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    if text_seed is None:\n",
    "        text_seed = ['<s>']*(model.order-1)\n",
    "    \n",
    "    content = [tok for tok in text_seed if tok!='<s>']\n",
    "    \n",
    "    for token in model.generate(num_words=max_words, text_seed=text_seed, random_seed=random_seed):\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        if token != '<s>':\n",
    "            content.append(token)\n",
    "        \n",
    "    tweet = tweet_detokenizer(content)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "523e7c2373f6a4c4b6a542f1e82369fedb6cbd21"
   },
   "outputs": [],
   "source": [
    "generate_tweet(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "523e7c2373f6a4c4b6a542f1e82369fedb6cbd21"
   },
   "outputs": [],
   "source": [
    "generate_tweet(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "523e7c2373f6a4c4b6a542f1e82369fedb6cbd21"
   },
   "outputs": [],
   "source": [
    "generate_tweet(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83a71e4b25b53795dedce91c1a5686b7e22279e0"
   },
   "source": [
    "**To go further:** We see in some generations some weird typos or tokens that probably did not occur often in the training data overall, and we might want to ignore.\n",
    "\n",
    "You can tell the vocabulary to ignore such words using the `unk_cutoff` argument for the vocabulary lookup, which will turn them to `'<UNK>'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = nltk.lm.Vocabulary(unk_cutoff=?)\n",
    "voc.update([\"a\",\"b\",\"a\"])\n",
    "voc.lookup([\"a\",\"b\",\"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc[\"a\"], voc[\"b\"], voc[\"c\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you are interested in the implementation and going a bit further, you can check out the documentation for the `nltk.lm.vocabulary.Vocabulary` class [here](https://www.nltk.org/api/nltk.lm.vocabulary.html) or the source code: [`nltk.lm.vocabulary.Vocabulary`](https://github.com/nltk/nltk/blob/develop/nltk/lm/vocabulary.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0310a2f3c7d9e90f4574b44838336ca1e4ef2ef"
   },
   "source": [
    "As discussed in the lecture, the issue of the simple MLE is that it gives 0 probability to any sequence for which even a single trigram has never been seen during the training. To avoid this issue, several smoothing techniques exist. A few implementations are available in the `nltk.lm` submodule, for example:\n",
    "\n",
    " - `Lidstone`: Provides Lidstone-smoothed scores.\n",
    " - `Laplace`: Implements Laplace (add one) smoothing. Equivalent to Lidstone with gamma=1.\n",
    " - `InterpolatedLanguageModel`: Logic common to all interpolated language models (Chen & Goodman 1995).\n",
    " - `WittenBellInterpolated`: Interpolated version of Witten-Bell smoothing.\n",
    " \n",
    "Let's fit the Laplace model introduced in the lecture, as well as its Lindstone generalization, that performs smoothing by adding an arbitrary value `gamma` instead of `1` to the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace, Lidstone\n",
    "laplace = ??\n",
    "lidstone = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_tweet(laplace, max_words=100, text_seed=[\"Donald\", \"Trump\"], random_seed=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_tweet(lidstone, max_words=100, text_seed=[\"Donald\", \"Trump\"], random_seed=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative effects of n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to visualize the impact of the n-gram order on the realism of the generated tweets, we can fit and generate from MLE and Laplace models with different orders (for ex. $n=1,2,3,4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model perplexity is a normalized form of the sequence probability, as seen in the lecture. It can be used on a kept-aside test dataset to evaluate the performance of a ngram probability model. The `nltk.lm.model` classes have a `.perplexity()` method to compute the perplexity on a list of ngrams.\n",
    "\n",
    "We can use it to compare the MLE, Laplace and Lindstone (e.g. with $\\gamma=1$) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(lm_model, test_corpus_tokens, order=None):\n",
    "    if order is None:\n",
    "        order=lm_model.order\n",
    "    \n",
    "    test_ngrams = []\n",
    "    for s in test_corpus_tokens:\n",
    "        test_ngrams += list(ngrams(pad_both_ends(s, n=order), n=order))\n",
    "    return lm_model.perplexity(test_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "???\n",
    "???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
