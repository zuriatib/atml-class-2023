{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Jwv4NQIZQx3"
   },
   "source": [
    "# NLP Seminar 2 - Normalization and Simple Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first NLP seminar, we focus on classical text normalization methods (stemming, lemmatization) and simple word embedding/vectorization techniques (bag of words, TF-IDF), that will allow us to train machine learning methods on text data. We use again the `nltk` (natural language toolkit) python package, as well as `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1946,
     "status": "ok",
     "timestamp": 1662019861618,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "ZSzIHTvFUnOn"
   },
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 999,
     "status": "ok",
     "timestamp": 1662019862614,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "-AtJGJBCWZpo",
    "outputId": "1f533aca-94fc-4526-b7f7-18af2bbce299"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First download some nltk resources\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"omw-1.4\")\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"averaged_perceptron_tagger\")\n",
    "# alternatively, they are all part of:\n",
    "nltk.download('popular', quiet=True)\n",
    "nltk.download('universal_tagset', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8pxK9ucU-LX"
   },
   "source": [
    "# 1. Download the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the famous `20newsgroups` dataset. It consists of a large collection of news posts across 20 topics. We will be using it to test some basic NLP techniques and train a multi-class classification model to predict the most likely topic for unseen news posts. \n",
    "For more information, check [the dataset description](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset) and the [import function helper](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)\n",
    "\n",
    "To make the task a bit harder, we import the data without the headers, footers and quotes.\n",
    "\n",
    "We also restrict the dataset to only 4 of the categories, for presentation simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 19957,
     "status": "ok",
     "timestamp": 1662019882569,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "VBE9MVJDTSHK"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# we restrict the data to the following response categories:\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "data_train = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42,\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset=\"test\", categories=categories, shuffle=True, random_state=42,\n",
    "                               remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "\n",
    "df_train = pd.DataFrame({\"text\": data_train.data, \"class\": data_train.target})\n",
    "df_test = pd.DataFrame({\"text\": data_test.data, \"class\": data_test.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1662019882569,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "yaOYzXrqUmTY",
    "outputId": "10c2ff36-5713-4bef-ca6d-afec8797c10a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does anyone know of a good way (standard PC ap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi,\\n\\n\\tI have a problem, I hope some of the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Well, I'll email also, but this may apply to ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello,\\n\\nI'm writing a paper on the role of t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class\n",
       "0  Does anyone know of a good way (standard PC ap...      1\n",
       "1  Hi,\\n\\n\\tI have a problem, I hope some of the ...      1\n",
       "2  (Well, I'll email also, but this may apply to ...      3\n",
       "3  Hello,\\n\\nI'm writing a paper on the role of t...      3\n",
       "4                                                         3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1662019882569,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "Gq_er6VLuSTE",
    "outputId": "0698f796-4d3f-4fcd-ec8a-6d4c3d4c10fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many classes are there to identify?\n",
    "df_train[\"class\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1662019882569,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "fPPcy8rpuYmF",
    "outputId": "e5cf7ee6-cf81-4849-80e5-5f581d05af79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n"
     ]
    }
   ],
   "source": [
    "# What do these class labels correspond to?\n",
    "target_names = data_train.target_names\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1662019882569,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "Ck87gAPDWv-f",
    "outputId": "cf91124d-94d3-46e5-800f-a8cf2df37b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2257, 2)\n",
      "Test: (1502, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Test: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1uc0__Px0R5"
   },
   "source": [
    "# 2. Natural Language Processing: Text Normalization\n",
    "\n",
    "For classic NLP techniques, text normalization can be crucial to good model performance. Their main aim is to decrease the vocabulary size, by reducing similar words to common roots or removing useless words. These techniques include word tokenization, stopword removal, lemmatization, and more. We'll try a few of those here to see their impact on our news classification model.\n",
    "\n",
    "We begin by testing those text normalization methods on a single news post example, before applying them to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1662019882569,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "NWWF9t3S0OZo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi:\n",
      "\n",
      "I am digitizing a NTSC signal and displaying on a PC video monitor.\n",
      "It is known that the display response of tubes is non-linear and is\n",
      "sometimes said to follow Gamma-Law. I am not certain if these\n",
      "non-linearities are \"Gamma-corrected\" before encoding NTSC signals\n",
      "or if the TV display is supposed to correct this.\n",
      " \n",
      "Also, if  256 grey levels, for example, are coded in a C program do\n",
      "these intensity levels appear with linear brightness on a PC\n",
      "monitor? In other words does PC monitor display circuitry\n",
      "correct for \"gamma errrors\"?\n",
      " \n",
      "Your response is much appreciated.\n",
      " \n",
      "Amjad.\n"
     ]
    }
   ],
   "source": [
    "# Take an example entry in the training set for demonstration purposes here\n",
    "text = df_train[\"text\"].iloc[11]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv7hitczyvUA"
   },
   "source": [
    "### 2.1. Tokenization\n",
    "Before any type of normalization, the first step is to tokenize each document in our corpus. We again rely on the NLTK tokenizer to convert a string to a list of words. We here tokennize the selected `text` observation from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1662019882570,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "pfbCYh3Jyu5z",
    "outputId": "b7c341ad-f608-4335-9d3b-c5de97c12b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ':', 'I', 'am', 'digitizing', 'a', 'NTSC', 'signal', 'and', 'displaying', 'on', 'a', 'PC', 'video', 'monitor', '.', 'It', 'is', 'known', 'that', 'the', 'display', 'response', 'of', 'tubes', 'is', 'non-linear', 'and', 'is', 'sometimes', 'said', 'to', 'follow', 'Gamma-Law', '.', 'I', 'am', 'not', 'certain', 'if', 'these', 'non-linearities', 'are', '``', 'Gamma-corrected', \"''\", 'before', 'encoding', 'NTSC', 'signals', 'or', 'if', 'the', 'TV', 'display', 'is', 'supposed', 'to', 'correct', 'this', '.', 'Also', ',', 'if', '256', 'grey', 'levels', ',', 'for', 'example', ',', 'are', 'coded', 'in', 'a', 'C', 'program', 'do', 'these', 'intensity', 'levels', 'appear', 'with', 'linear', 'brightness', 'on', 'a', 'PC', 'monitor', '?', 'In', 'other', 'words', 'does', 'PC', 'monitor', 'display', 'circuitry', 'correct', 'for', '``', 'gamma', 'errrors', \"''\", '?', 'Your', 'response', 'is', 'much', 'appreciated', '.', 'Amjad', '.']\n"
     ]
    }
   ],
   "source": [
    "text_tokens = nltk.word_tokenize(text)\n",
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfzsMmdIymLR"
   },
   "source": [
    "### 2.2. Stopwords and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5umdlOizLCp"
   },
   "source": [
    "Stopwords correspond to unimportant words which might safely be ignored for the task at hand. For the specific task of news classification, we might for example rely on the default stopwords provided by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1662019882923,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "QFikvQzWUkq7",
    "outputId": "186cfccc-ddf4-4e77-e5d5-7a98d90eb80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Let's use the nltk stopwords available for the English language\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might also want to remove punctuation tokens. Here is a list of standard punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to reducing similar words to common roots, stemming is the simplest and fastest approach. Stemming is an abstract rule-based process that stems or removes some of the last few characters from a word. This sometimes leads to incorrect meanings and spelling, as a downsize.\n",
    "\n",
    "Let's try two different stemmers: \"PorterStemmer\" and the slightly more recent \"SnowballStemmer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', ':', 'i', 'am', 'digit', 'a', 'ntsc', 'signal', 'and', 'display', 'on', 'a', 'pc', 'video', 'monitor', '.', 'it', 'is', 'known', 'that', 'the', 'display', 'respons', 'of', 'tube', 'is', 'non-linear', 'and', 'is', 'sometim', 'said', 'to', 'follow', 'gamma-law', '.', 'i', 'am', 'not', 'certain', 'if', 'these', 'non-linear', 'are', '``', 'gamma-correct', \"''\", 'befor', 'encod', 'ntsc', 'signal', 'or', 'if', 'the', 'tv', 'display', 'is', 'suppos', 'to', 'correct', 'thi', '.', 'also', ',', 'if', '256', 'grey', 'level', ',', 'for', 'exampl', ',', 'are', 'code', 'in', 'a', 'c', 'program', 'do', 'these', 'intens', 'level', 'appear', 'with', 'linear', 'bright', 'on', 'a', 'pc', 'monitor', '?', 'in', 'other', 'word', 'doe', 'pc', 'monitor', 'display', 'circuitri', 'correct', 'for', '``', 'gamma', 'errror', \"''\", '?', 'your', 'respons', 'is', 'much', 'appreci', '.', 'amjad', '.']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "text_stems = [ps.stem(word) for word in text_tokens]\n",
    "print(text_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', ':', 'i', 'am', 'digit', 'a', 'ntsc', 'signal', 'and', 'display', 'on', 'a', 'pc', 'video', 'monitor', '.', 'it', 'is', 'known', 'that', 'the', 'display', 'respons', 'of', 'tube', 'is', 'non-linear', 'and', 'is', 'sometim', 'said', 'to', 'follow', 'gamma-law', '.', 'i', 'am', 'not', 'certain', 'if', 'these', 'non-linear', 'are', '``', 'gamma-correct', \"''\", 'befor', 'encod', 'ntsc', 'signal', 'or', 'if', 'the', 'tv', 'display', 'is', 'suppos', 'to', 'correct', 'this', '.', 'also', ',', 'if', '256', 'grey', 'level', ',', 'for', 'exampl', ',', 'are', 'code', 'in', 'a', 'c', 'program', 'do', 'these', 'intens', 'level', 'appear', 'with', 'linear', 'bright', 'on', 'a', 'pc', 'monitor', '?', 'in', 'other', 'word', 'doe', 'pc', 'monitor', 'display', 'circuitri', 'correct', 'for', '``', 'gamma', 'errror', \"''\", '?', 'your', 'respons', 'is', 'much', 'appreci', '.', 'amjad', '.']\n"
     ]
    }
   ],
   "source": [
    "snowstem = SnowballStemmer(\"english\")\n",
    "text_sstems = [snowstem.stem(word) for word in text_tokens]\n",
    "print(text_sstems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi -> hi\n",
      ": -> :\n",
      "I -> i\n",
      "am -> am\n",
      "digitizing -> digit\n",
      "a -> a\n",
      "NTSC -> ntsc\n",
      "signal -> signal\n",
      "and -> and\n",
      "displaying -> display\n",
      "on -> on\n",
      "a -> a\n",
      "PC -> pc\n",
      "video -> video\n",
      "monitor -> monitor\n",
      ". -> .\n",
      "It -> it\n",
      "is -> is\n",
      "known -> known\n",
      "that -> that\n",
      "the -> the\n",
      "display -> display\n",
      "response -> respons\n",
      "of -> of\n",
      "tubes -> tube\n",
      "is -> is\n",
      "non-linear -> non-linear\n",
      "and -> and\n",
      "is -> is\n",
      "sometimes -> sometim\n",
      "said -> said\n",
      "to -> to\n",
      "follow -> follow\n",
      "Gamma-Law -> gamma-law\n",
      ". -> .\n",
      "I -> i\n",
      "am -> am\n",
      "not -> not\n",
      "certain -> certain\n",
      "if -> if\n",
      "these -> these\n",
      "non-linearities -> non-linear\n",
      "are -> are\n",
      "`` -> ``\n",
      "Gamma-corrected -> gamma-correct\n",
      "'' -> ''\n",
      "before -> befor\n",
      "encoding -> encod\n",
      "NTSC -> ntsc\n",
      "signals -> signal\n",
      "or -> or\n",
      "if -> if\n",
      "the -> the\n",
      "TV -> tv\n",
      "display -> display\n",
      "is -> is\n",
      "supposed -> suppos\n",
      "to -> to\n",
      "correct -> correct\n",
      "this -> this\n",
      ". -> .\n",
      "Also -> also\n",
      ", -> ,\n",
      "if -> if\n",
      "256 -> 256\n",
      "grey -> grey\n",
      "levels -> level\n",
      ", -> ,\n",
      "for -> for\n",
      "example -> exampl\n",
      ", -> ,\n",
      "are -> are\n",
      "coded -> code\n",
      "in -> in\n",
      "a -> a\n",
      "C -> c\n",
      "program -> program\n",
      "do -> do\n",
      "these -> these\n",
      "intensity -> intens\n",
      "levels -> level\n",
      "appear -> appear\n",
      "with -> with\n",
      "linear -> linear\n",
      "brightness -> bright\n",
      "on -> on\n",
      "a -> a\n",
      "PC -> pc\n",
      "monitor -> monitor\n",
      "? -> ?\n",
      "In -> in\n",
      "other -> other\n",
      "words -> word\n",
      "does -> doe\n",
      "PC -> pc\n",
      "monitor -> monitor\n",
      "display -> display\n",
      "circuitry -> circuitri\n",
      "correct -> correct\n",
      "for -> for\n",
      "`` -> ``\n",
      "gamma -> gamma\n",
      "errrors -> errror\n",
      "'' -> ''\n",
      "? -> ?\n",
      "Your -> your\n",
      "response -> respons\n",
      "is -> is\n",
      "much -> much\n",
      "appreciated -> appreci\n",
      ". -> .\n",
      "Amjad -> amjad\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "for a,b in zip(text_tokens, text_sstems):\n",
    "    print(a, \"->\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 'this')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(a,b) for a, b in zip(text_stems, text_sstems) if a!=b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJnX396HzwRM"
   },
   "source": [
    "### 2.4. Part-of-speech (POS) tagging\n",
    "Part-of-speech tagging categorizes words as a particular part-of-speech (e.g. verb, noun, etc ...) using the word itself and the surrounding context in a predictive model. Those tags can be useful for several types of analyses. In particular, they are used for lemmatization.\n",
    "\n",
    "Different more or less precise tagsets exist for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1662019882923,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "EnbOxzaCzvtw",
    "outputId": "33bd40b9-cfe3-461a-cf2d-7db45c8b48a5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'NN'), (':', ':'), ('I', 'PRP'), ('am', 'VBP'), ('digitizing', 'VBG'), ('a', 'DT'), ('NTSC', 'NNP'), ('signal', 'NN'), ('and', 'CC'), ('displaying', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('PC', 'NN'), ('video', 'NN'), ('monitor', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('known', 'VBN'), ('that', 'IN'), ('the', 'DT'), ('display', 'NN'), ('response', 'NN'), ('of', 'IN'), ('tubes', 'NNS'), ('is', 'VBZ'), ('non-linear', 'JJ'), ('and', 'CC'), ('is', 'VBZ'), ('sometimes', 'RB'), ('said', 'VBD'), ('to', 'TO'), ('follow', 'VB'), ('Gamma-Law', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('not', 'RB'), ('certain', 'JJ'), ('if', 'IN'), ('these', 'DT'), ('non-linearities', 'NNS'), ('are', 'VBP'), ('``', '``'), ('Gamma-corrected', 'JJ'), (\"''\", \"''\"), ('before', 'IN'), ('encoding', 'VBG'), ('NTSC', 'NNP'), ('signals', 'NNS'), ('or', 'CC'), ('if', 'IN'), ('the', 'DT'), ('TV', 'NN'), ('display', 'NN'), ('is', 'VBZ'), ('supposed', 'VBN'), ('to', 'TO'), ('correct', 'VB'), ('this', 'DT'), ('.', '.'), ('Also', 'RB'), (',', ','), ('if', 'IN'), ('256', 'CD'), ('grey', 'NN'), ('levels', 'NNS'), (',', ','), ('for', 'IN'), ('example', 'NN'), (',', ','), ('are', 'VBP'), ('coded', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('C', 'NNP'), ('program', 'NN'), ('do', 'VBP'), ('these', 'DT'), ('intensity', 'NN'), ('levels', 'NNS'), ('appear', 'VBP'), ('with', 'IN'), ('linear', 'JJ'), ('brightness', 'NN'), ('on', 'IN'), ('a', 'DT'), ('PC', 'NN'), ('monitor', 'NN'), ('?', '.'), ('In', 'IN'), ('other', 'JJ'), ('words', 'NNS'), ('does', 'VBZ'), ('PC', 'NNP'), ('monitor', 'NN'), ('display', 'NN'), ('circuitry', 'NN'), ('correct', 'NN'), ('for', 'IN'), ('``', '``'), ('gamma', 'JJ'), ('errrors', 'NNS'), (\"''\", \"''\"), ('?', '.'), ('Your', 'PRP$'), ('response', 'NN'), ('is', 'VBZ'), ('much', 'RB'), ('appreciated', 'VBN'), ('.', '.'), ('Amjad', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# We can use the default pos-tagger provided by NLTK\n",
    "text_PoS_p = nltk.pos_tag(text_tokens)\n",
    "print(text_PoS_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1662019882923,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "EnbOxzaCzvtw",
    "outputId": "33bd40b9-cfe3-461a-cf2d-7db45c8b48a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'NOUN'), (':', '.'), ('I', 'PRON'), ('am', 'VERB'), ('digitizing', 'VERB'), ('a', 'DET'), ('NTSC', 'NOUN'), ('signal', 'NOUN'), ('and', 'CONJ'), ('displaying', 'VERB'), ('on', 'ADP'), ('a', 'DET'), ('PC', 'NOUN'), ('video', 'NOUN'), ('monitor', 'NOUN'), ('.', '.'), ('It', 'PRON'), ('is', 'VERB'), ('known', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('display', 'NOUN'), ('response', 'NOUN'), ('of', 'ADP'), ('tubes', 'NOUN'), ('is', 'VERB'), ('non-linear', 'ADJ'), ('and', 'CONJ'), ('is', 'VERB'), ('sometimes', 'ADV'), ('said', 'VERB'), ('to', 'PRT'), ('follow', 'VERB'), ('Gamma-Law', 'NOUN'), ('.', '.'), ('I', 'PRON'), ('am', 'VERB'), ('not', 'ADV'), ('certain', 'ADJ'), ('if', 'ADP'), ('these', 'DET'), ('non-linearities', 'NOUN'), ('are', 'VERB'), ('``', '.'), ('Gamma-corrected', 'ADJ'), (\"''\", '.'), ('before', 'ADP'), ('encoding', 'VERB'), ('NTSC', 'NOUN'), ('signals', 'NOUN'), ('or', 'CONJ'), ('if', 'ADP'), ('the', 'DET'), ('TV', 'NOUN'), ('display', 'NOUN'), ('is', 'VERB'), ('supposed', 'VERB'), ('to', 'PRT'), ('correct', 'VERB'), ('this', 'DET'), ('.', '.'), ('Also', 'ADV'), (',', '.'), ('if', 'ADP'), ('256', 'NUM'), ('grey', 'NOUN'), ('levels', 'NOUN'), (',', '.'), ('for', 'ADP'), ('example', 'NOUN'), (',', '.'), ('are', 'VERB'), ('coded', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('C', 'NOUN'), ('program', 'NOUN'), ('do', 'VERB'), ('these', 'DET'), ('intensity', 'NOUN'), ('levels', 'NOUN'), ('appear', 'VERB'), ('with', 'ADP'), ('linear', 'ADJ'), ('brightness', 'NOUN'), ('on', 'ADP'), ('a', 'DET'), ('PC', 'NOUN'), ('monitor', 'NOUN'), ('?', '.'), ('In', 'ADP'), ('other', 'ADJ'), ('words', 'NOUN'), ('does', 'VERB'), ('PC', 'NOUN'), ('monitor', 'NOUN'), ('display', 'NOUN'), ('circuitry', 'NOUN'), ('correct', 'NOUN'), ('for', 'ADP'), ('``', '.'), ('gamma', 'ADJ'), ('errrors', 'NOUN'), (\"''\", '.'), ('?', '.'), ('Your', 'PRON'), ('response', 'NOUN'), ('is', 'VERB'), ('much', 'ADV'), ('appreciated', 'VERB'), ('.', '.'), ('Amjad', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Or the \"universal\" tags\n",
    "text_PoS_u = nltk.pos_tag(text_tokens, tagset=\"universal\")\n",
    "print(text_PoS_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf\n",
    "\n",
    "https://github.com/slavpetrov/universal-pos-tags\n",
    "\n",
    "- VERB - verbs (all tenses and modes)\n",
    "- NOUN - nouns (common and proper)\n",
    "- PRON - pronouns \n",
    "- ADJ - adjectives\n",
    "- ADV - adverbs\n",
    "- ADP - adpositions (prepositions and postpositions)\n",
    "- CONJ - conjunctions\n",
    "- DET - determiners\n",
    "- NUM - cardinal numbers\n",
    "- PRT - particles or other function words\n",
    "- X - other: foreign words, typos, abbreviations\n",
    "- . - punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7byMSxD0yq0O"
   },
   "source": [
    "### 2.5. Lemmatization\n",
    "Lemmatization considers the context and converts the word to its meaningful base form, which is called the \"Lemma\".\n",
    "It uses language-specific lookup tables to find the root forms of words. This makes it more computationnaly expensive than stemming.\n",
    "We here use the `WordNetLemmatizer` (https://www.nltk.org/api/nltk.stem.wordnet.html).\n",
    "It uses the Wordnet database (https://wordnet.princeton.edu/) to find the lemma of a word, given the word AND its PoS tag!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662019882923,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "P_DEWPixwFcB"
   },
   "outputs": [],
   "source": [
    "# We can use the English language lemmatizer provided by NLTK\n",
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "?wnl.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1948,
     "status": "ok",
     "timestamp": 1662019884869,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "wCXSxkwIwKvh",
    "outputId": "89c7f2f4-9103-4bf0-823e-b1d5fa3dc41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi -> Hi\n",
      ": -> :\n",
      "I -> I\n",
      "am -> be\n",
      "digitizing -> digitize\n",
      "a -> a\n",
      "NTSC -> NTSC\n",
      "signal -> signal\n",
      "and -> and\n",
      "displaying -> display\n",
      "on -> on\n",
      "a -> a\n",
      "PC -> PC\n",
      "video -> video\n",
      "monitor -> monitor\n",
      ". -> .\n",
      "It -> It\n",
      "is -> be\n",
      "known -> know\n",
      "that -> that\n",
      "the -> the\n",
      "display -> display\n",
      "response -> response\n",
      "of -> of\n",
      "tubes -> tube\n",
      "is -> be\n",
      "non-linear -> non-linear\n",
      "and -> and\n",
      "is -> be\n",
      "sometimes -> sometimes\n",
      "said -> say\n",
      "to -> to\n",
      "follow -> follow\n",
      "Gamma-Law -> Gamma-Law\n",
      ". -> .\n",
      "I -> I\n",
      "am -> be\n",
      "not -> not\n",
      "certain -> certain\n",
      "if -> if\n",
      "these -> these\n",
      "non-linearities -> non-linearities\n",
      "are -> be\n",
      "`` -> ``\n",
      "Gamma-corrected -> Gamma-corrected\n",
      "'' -> ''\n",
      "before -> before\n",
      "encoding -> encode\n",
      "NTSC -> NTSC\n",
      "signals -> signal\n",
      "or -> or\n",
      "if -> if\n",
      "the -> the\n",
      "TV -> TV\n",
      "display -> display\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "to -> to\n",
      "correct -> correct\n",
      "this -> this\n",
      ". -> .\n",
      "Also -> Also\n",
      ", -> ,\n",
      "if -> if\n",
      "256 -> 256\n",
      "grey -> grey\n",
      "levels -> level\n",
      ", -> ,\n",
      "for -> for\n",
      "example -> example\n",
      ", -> ,\n",
      "are -> be\n",
      "coded -> cod\n",
      "in -> in\n",
      "a -> a\n",
      "C -> C\n",
      "program -> program\n",
      "do -> do\n",
      "these -> these\n",
      "intensity -> intensity\n",
      "levels -> level\n",
      "appear -> appear\n",
      "with -> with\n",
      "linear -> linear\n",
      "brightness -> brightness\n",
      "on -> on\n",
      "a -> a\n",
      "PC -> PC\n",
      "monitor -> monitor\n",
      "? -> ?\n",
      "In -> In\n",
      "other -> other\n",
      "words -> word\n",
      "does -> do\n",
      "PC -> PC\n",
      "monitor -> monitor\n",
      "display -> display\n",
      "circuitry -> circuitry\n",
      "correct -> correct\n",
      "for -> for\n",
      "`` -> ``\n",
      "gamma -> gamma\n",
      "errrors -> errrors\n",
      "'' -> ''\n",
      "? -> ?\n",
      "Your -> Your\n",
      "response -> response\n",
      "is -> be\n",
      "much -> much\n",
      "appreciated -> appreciate\n",
      ". -> .\n",
      "Amjad -> Amjad\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "for w, pos in text_PoS_u:\n",
    "    # Map the detailed set of POS-tags to nouns/verbs for the lemmatizer\n",
    "    if pos in [\"VERB\"]:\n",
    "        pos = \"v\"\n",
    "    elif pos in [\"ADJ\"]:\n",
    "        pos = \"a\"\n",
    "    elif pos in [\"ADV\"]:\n",
    "        pos = \"r\"\n",
    "    elif pos in [\"NOUN\"]:\n",
    "        pos = \"n\"\n",
    "    else:\n",
    "        pos = \"n\"\n",
    "    # print the original tokens and their lemma\n",
    "    print(f\"{w} -> {wnl.lemmatize(w, pos=pos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run - stem: run - lemma: run\n",
      "ran - stem: ran - lemma: run\n",
      "universal - stem: univers - lemma: universal\n",
      "university - stem: univers - lemma: university\n",
      "universe - stem: univers - lemma: universe\n",
      "alumnus - stem: alumnu - lemma: alumnus\n",
      "alumni - stem: alumni - lemma: alumnus\n"
     ]
    }
   ],
   "source": [
    "# To illustrate the difference:\n",
    "diff_ex = [\"run\",\"ran\",\n",
    "           \"universal\", \"university\", \"universe\",\n",
    "           \"alumnus\",\"alumni\"]\n",
    "\n",
    "for w, pos in nltk.pos_tag(diff_ex, tagset=\"universal\"):\n",
    "    # Map the detailed set of POS-tags to nouns/verbs for the lemmatizer\n",
    "    if pos in [\"VERB\"]:\n",
    "        pos = \"v\"\n",
    "    elif pos in [\"ADJ\"]:\n",
    "        pos = \"a\"\n",
    "    elif pos in [\"ADV\"]:\n",
    "        pos = \"r\"\n",
    "    elif pos in [\"NOUN\"]:\n",
    "        pos = \"n\"\n",
    "    else:\n",
    "        pos = \"n\"\n",
    "    print(f\"{w} - stem: {ps.stem(w)} - lemma: {wnl.lemmatize(w, pos=pos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfUp_mj32YHg"
   },
   "source": [
    "### 2.6. Putting it all together\n",
    "\n",
    "Let's combine all of the above techniques into a single \"tokenize + normalize\" function which will output a list of normalized words given a string of text (document) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1662019884869,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "KzwaS4cSvr52"
   },
   "outputs": [],
   "source": [
    "# Write your custom tokenizer method here:\n",
    "def custom_tokenizer(text: str):\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    output = []\n",
    "    for w, pos in nltk.pos_tag(text_tokens, tagset=\"universal\"):\n",
    "        if pos in [\"VERB\"]:\n",
    "            pos = \"v\"\n",
    "        elif pos in [\"ADJ\"]:\n",
    "            pos = \"a\"\n",
    "        elif pos in [\"ADV\"]:\n",
    "            pos = \"r\"\n",
    "        elif pos in [\"NOUN\"]:\n",
    "            pos = \"n\"\n",
    "        else:\n",
    "            pos = \"n\"\n",
    "\n",
    "        # Lemmatized form accounting for POS-tag\n",
    "        l = wnl.lemmatize(w, pos=pos)\n",
    "\n",
    "        # Filter out stopwords\n",
    "        if l not in stopwords:\n",
    "            output.append(l)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1662019884870,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "m7Fet-3W3ddO",
    "outputId": "983bc229-d2e4-4066-c07a-4312ebd5a8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ':', 'I', 'digitize', 'NTSC', 'signal', 'display', 'PC', 'video', 'monitor', '.', 'It', 'know', 'display', 'response', 'tube', 'non-linear', 'sometimes', 'say', 'follow', 'Gamma-Law', '.', 'I', 'certain', 'non-linearities', '``', 'Gamma-corrected', \"''\", 'encode', 'NTSC', 'signal', 'TV', 'display', 'suppose', 'correct', '.', 'Also', ',', '256', 'grey', 'level', ',', 'example', ',', 'cod', 'C', 'program', 'intensity', 'level', 'appear', 'linear', 'brightness', 'PC', 'monitor', '?', 'In', 'word', 'PC', 'monitor', 'display', 'circuitry', 'correct', '``', 'gamma', 'errrors', \"''\", '?', 'Your', 'response', 'much', 'appreciate', '.', 'Amjad', '.']\n"
     ]
    }
   ],
   "source": [
    "# Try it out on the text\n",
    "print(custom_tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfxFjHPM3jrQ"
   },
   "source": [
    "# 3. Word embeddings and text classification\n",
    "\n",
    "Using the NLP normalization techniques from above, including our custom tokenization method, we want to train a simple multi-class ML classifier to predict the news topic. In order to do so, we must first vectorize the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNIGZbIM32VC"
   },
   "source": [
    "### 3.1. Bag-of-words (BOW)\n",
    "\n",
    "Bag-of-words is the simplest *embedding* technique in which words are represented as one-hot encoded numeric vectors of word counts. The length of these vectors corresponds to the size of the (reduced) vocabulary of the training corpus.\n",
    "That is, each column $j$ in the resulting sparse matrix represents a word and and each row $i$ represents a different observation (i.e. document), and the coresonping entry in the matrix is the word count of how many times the word $j$ appears in document $i$.\n",
    "See [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for the scikit-learn implementation of the bag-of-words embedding and its many options.\n",
    "\n",
    "We start by training the bag-of-words on our train corpus, and compare the vocabulary size with and without our `'custom_tokenizer'` normalization procedure. We can ignore tokens that appear in less than 0.1% of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_vectorizer = CountVectorizer(lowercase=True, min_df=1e-3)\n",
    "\n",
    "bow_train_simple = simple_vectorizer.fit_transform(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 9187)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 174431,
     "status": "ok",
     "timestamp": 1662020059298,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "v5A-Xfxl6wYb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pascheo\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer, lowercase=True, min_df=1e-3)\n",
    "\n",
    "bow_train = vectorizer.fit_transform(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 7245)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss1gb3fi9svt"
   },
   "source": [
    "### 3.2. Bag-of-words (BOW) with Term-frequency Inverse-document-frequency (TF-IDF)\n",
    "\n",
    "TF-IDF uses the **same** one-hot encoding as traditional BOW, but transforms the simple counts to the relative word frequency, normalized by the inverse-document-frequency to account for frequently occurring words across all documents. \n",
    "\n",
    "The intuition is that not only the word's frequency in a given document indicates if that word represents the document well, but also how rare it is in other documents in comparison.\n",
    "\n",
    "We now compute the TF-IDF transformation of the BoW, using our `'custom_tokenizer'`. See the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) scikit-learn classes for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Way 1: two-step bag of words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "#using the BoW from above:\n",
    "tfidf_train0 = tfidf_transformer.fit_transform(bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 7245)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Way 2: both at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we repeat the two steps above in a single equivalent step, using TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, lowercase=True, min_df=1e-3)\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 7245)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Topic Classification using Simple Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Lemmatization and TF-IDF impacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we want to study the impact of lemmatization and of TF-IDF on the classification accuracy of a simple ML model. We use a multinomial logistic regression with default hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the test accuracy using BoW without lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1662020167450,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "k5fRMDy-XTZA",
    "outputId": "797ea1f6-ba41-482f-e3a1-18f10ab828dc"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=int(1e4))\n",
    "clf.fit(bow_train_simple, df_train[\"class\"])\n",
    "\n",
    "# Run the inference on test data\n",
    "pred_bow_simple = clf.predict(simple_vectorizer.transform(df_test[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1662020167450,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "k5fRMDy-XTZA",
    "outputId": "797ea1f6-ba41-482f-e3a1-18f10ab828dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7496671105193076"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report the score\n",
    "accuracy_score(df_test[\"class\"], pred_bow_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the lemmatization improving the test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1662020167450,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "k5fRMDy-XTZA",
    "outputId": "797ea1f6-ba41-482f-e3a1-18f10ab828dc"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=int(1e4))\n",
    "clf.fit(bow_train, df_train[\"class\"])\n",
    "\n",
    "# Run the inference on test data\n",
    "pred_bow = clf.predict(vectorizer.transform(df_test[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1662020167450,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "k5fRMDy-XTZA",
    "outputId": "797ea1f6-ba41-482f-e3a1-18f10ab828dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.762982689747004"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report the score\n",
    "accuracy_score(df_test[\"class\"], pred_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oQxaBxksixp"
   },
   "source": [
    "Let's see if using TF-IDF weights improves the score further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1662020451269,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "ABuLdR_KtNMB",
    "outputId": "b149ebfd-8841-4d8f-af90-a14646920f97"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=int(1e4))\n",
    "clf.fit(tfidf_train, df_train[\"class\"])\n",
    "\n",
    "# Run the inference on test data\n",
    "pred_tfidf = clf.predict(tfidf_vectorizer.transform(df_test[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1662020451269,
     "user": {
      "displayName": "Zach Schillaci",
      "userId": "04144020147330636013"
     },
     "user_tz": -120
    },
    "id": "ABuLdR_KtNMB",
    "outputId": "b149ebfd-8841-4d8f-af90-a14646920f97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7902796271637816"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report the score\n",
    "accuracy_score(df_test[\"class\"], pred_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrNaJO09u4zR"
   },
   "source": [
    "This already performs better than a simple BOW model. You can try changing the vectorizer parameters or using a different ML model for the classification. We will investigate more advanced methods in later labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a cross-validated grid search to select the best hyper-parameter values for both the TF-IDF vectorizer and the logistic regression model at the same time using a sklearn `Pipeline`. For the TF-IDF Vectorizer, let's for example check if considering bigrams in the vocabulary helps the classifier. For the logistic regression, the tuning parameter is the cost ($1/penalty$) `'C'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(tokenizer=custom_tokenizer, lowercase=True, min_df=1e-3, max_df=0.999)),\n",
    "    (\"logistic\", LogisticRegression(max_iter=int(1e4)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pascheo\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=KFold(n_splits=3, random_state=42, shuffle=True),\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(min_df=0.001,\n",
       "                                                        tokenizer=&lt;function custom_tokenizer at 0x0000024C2DCEC1F0&gt;)),\n",
       "                                       (&#x27;logistic&#x27;,\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           multi_class=&#x27;multinomial&#x27;))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={&#x27;logistic__C&#x27;: [10, 100, 1000],\n",
       "                         &#x27;tfidf__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=KFold(n_splits=3, random_state=42, shuffle=True),\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(min_df=0.001,\n",
       "                                                        tokenizer=&lt;function custom_tokenizer at 0x0000024C2DCEC1F0&gt;)),\n",
       "                                       (&#x27;logistic&#x27;,\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           multi_class=&#x27;multinomial&#x27;))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={&#x27;logistic__C&#x27;: [10, 100, 1000],\n",
       "                         &#x27;tfidf__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(min_df=0.001,\n",
       "                                 tokenizer=&lt;function custom_tokenizer at 0x0000024C2DCEC1F0&gt;)),\n",
       "                (&#x27;logistic&#x27;,\n",
       "                 LogisticRegression(max_iter=10000,\n",
       "                                    multi_class=&#x27;multinomial&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(min_df=0.001,\n",
       "                tokenizer=&lt;function custom_tokenizer at 0x0000024C2DCEC1F0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=3, random_state=42, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(min_df=0.001,\n",
       "                                                        tokenizer=<function custom_tokenizer at 0x0000024C2DCEC1F0>)),\n",
       "                                       ('logistic',\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           multi_class='multinomial'))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={'logistic__C': [10, 100, 1000],\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "my_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    \"logistic__C\": [10, 100, 1000],\n",
    "}\n",
    "\n",
    "# Define folds\n",
    "folds = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Define grid search CV\n",
    "tfidf_logistic_cv = GridSearchCV(estimator=logistic_pipe, param_grid=my_grid, scoring=\"accuracy\", cv=folds, n_jobs=-2)\n",
    "\n",
    "# Run CV\n",
    "tfidf_logistic_cv.fit(df_train[\"text\"], df_train[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_logistic__C</th>\n",
       "      <th>param_tfidf__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.497286</td>\n",
       "      <td>7.672235</td>\n",
       "      <td>29.446819</td>\n",
       "      <td>2.391476</td>\n",
       "      <td>100</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'logistic__C': 100, 'tfidf__ngram_range': (1,...</td>\n",
       "      <td>0.868526</td>\n",
       "      <td>0.859043</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.859544</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.952068</td>\n",
       "      <td>1.381878</td>\n",
       "      <td>31.013361</td>\n",
       "      <td>1.099879</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'logistic__C': 10, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.864542</td>\n",
       "      <td>0.857713</td>\n",
       "      <td>0.849734</td>\n",
       "      <td>0.857330</td>\n",
       "      <td>0.006051</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.970096</td>\n",
       "      <td>13.564977</td>\n",
       "      <td>24.709396</td>\n",
       "      <td>3.421494</td>\n",
       "      <td>1000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'logistic__C': 1000, 'tfidf__ngram_range': (1...</td>\n",
       "      <td>0.861886</td>\n",
       "      <td>0.852394</td>\n",
       "      <td>0.845745</td>\n",
       "      <td>0.853341</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.562788</td>\n",
       "      <td>0.741009</td>\n",
       "      <td>33.600293</td>\n",
       "      <td>1.090875</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'logistic__C': 10, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.868526</td>\n",
       "      <td>0.847074</td>\n",
       "      <td>0.841755</td>\n",
       "      <td>0.852452</td>\n",
       "      <td>0.011572</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.983177</td>\n",
       "      <td>2.083764</td>\n",
       "      <td>27.871013</td>\n",
       "      <td>1.014869</td>\n",
       "      <td>100</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'logistic__C': 100, 'tfidf__ngram_range': (1,...</td>\n",
       "      <td>0.864542</td>\n",
       "      <td>0.847074</td>\n",
       "      <td>0.843085</td>\n",
       "      <td>0.851567</td>\n",
       "      <td>0.009318</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50.044275</td>\n",
       "      <td>2.023628</td>\n",
       "      <td>14.360304</td>\n",
       "      <td>1.448081</td>\n",
       "      <td>1000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'logistic__C': 1000, 'tfidf__ngram_range': (1...</td>\n",
       "      <td>0.860558</td>\n",
       "      <td>0.845745</td>\n",
       "      <td>0.844415</td>\n",
       "      <td>0.850239</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "2      59.497286      7.672235        29.446819        2.391476   \n",
       "0      45.952068      1.381878        31.013361        1.099879   \n",
       "4      60.970096     13.564977        24.709396        3.421494   \n",
       "1      52.562788      0.741009        33.600293        1.090875   \n",
       "3      74.983177      2.083764        27.871013        1.014869   \n",
       "5      50.044275      2.023628        14.360304        1.448081   \n",
       "\n",
       "  param_logistic__C param_tfidf__ngram_range  \\\n",
       "2               100                   (1, 1)   \n",
       "0                10                   (1, 1)   \n",
       "4              1000                   (1, 1)   \n",
       "1                10                   (1, 2)   \n",
       "3               100                   (1, 2)   \n",
       "5              1000                   (1, 2)   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "2  {'logistic__C': 100, 'tfidf__ngram_range': (1,...           0.868526   \n",
       "0  {'logistic__C': 10, 'tfidf__ngram_range': (1, 1)}           0.864542   \n",
       "4  {'logistic__C': 1000, 'tfidf__ngram_range': (1...           0.861886   \n",
       "1  {'logistic__C': 10, 'tfidf__ngram_range': (1, 2)}           0.868526   \n",
       "3  {'logistic__C': 100, 'tfidf__ngram_range': (1,...           0.864542   \n",
       "5  {'logistic__C': 1000, 'tfidf__ngram_range': (1...           0.860558   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "2           0.859043           0.851064         0.859544        0.007138   \n",
       "0           0.857713           0.849734         0.857330        0.006051   \n",
       "4           0.852394           0.845745         0.853341        0.006624   \n",
       "1           0.847074           0.841755         0.852452        0.011572   \n",
       "3           0.847074           0.843085         0.851567        0.009318   \n",
       "5           0.845745           0.844415         0.850239        0.007317   \n",
       "\n",
       "   rank_test_score  \n",
       "2                1  \n",
       "0                2  \n",
       "4                3  \n",
       "1                4  \n",
       "3                5  \n",
       "5                6  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_logistic_cv.cv_results_).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic__C': 100, 'tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_logistic_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tfidf_logistic_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = Pipeline([\n",
    "#    (\"tfidf\", TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,1), lowercase=True, min_df=1e-3, max_df=0.999)),\n",
    "#    (\"logistic\", LogisticRegression(C=100, multi_class=\"multinomial\", max_iter=int(1e4)))\n",
    "#])\n",
    "#best_model.fit(df_train[\"text\"], df_train[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8015978695073236"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = best_model.predict(df_test[\"text\"])\n",
    "accuracy_score(df_test[\"class\"], y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run some classification accuracy diagnostics to understand in more detail how our model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(df_test[\"class\"], y_hat)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data_test.target_names).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rows = actual category\n",
    "\n",
    "columns = predicted category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.72      0.65      0.68       319\n",
      "         comp.graphics       0.87      0.87      0.87       389\n",
      "               sci.med       0.80      0.87      0.83       396\n",
      "soc.religion.christian       0.80      0.79      0.79       398\n",
      "\n",
      "              accuracy                           0.80      1502\n",
      "             macro avg       0.80      0.79      0.79      1502\n",
      "          weighted avg       0.80      0.80      0.80      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test[\"class\"], y_hat, target_names=data_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision = $TP/(TP+FP)$ = When the model is predicting this class, how often it is right\n",
    "\n",
    "recall = $TP/(TP+FN)$ = How well the model is performing (accuracy) within each (true) class\n",
    "\n",
    "F1 = harmonic mean of precision and recall = $\\frac{2}{(1/precision)+(1/recall)}$ (1= best, prefect precision and recall, 0=worst, either precision or recall is zero)\n",
    "\n",
    "support = number of occurences in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the topic of a few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lungs and heart health => sci.med\n",
      "CPU or GPU? => alt.atheism\n",
      "God is love => soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['Lungs and heart health', \"CPU or GPU?\", 'God is love']\n",
    "\n",
    "predicted = best_model.predict(docs_new)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print(doc, \"=>\", data_test.target_names[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Text wrangling and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, textual datasets are rarely clean nor well structured, and often need some wrangling and preprocessing to be used effectively. \n",
    "\n",
    "Furthermore, depending on the specific task and context at hand, there are often other tailor-made transformations that can prove usefull as an addition or a replacement to normalization. (E.g. the way you would like to handle the `@`symbol might differ between e-mail and social media data. Or there might be specific groups of words that have similar meaning in general, but whose differentiation is important in a specific context.)\n",
    "\n",
    "Additionally to processing and normalizing the test for vectorisation, manual feature extraction can also prove useful. For example the number of exclamation marks, the number of ALL CAPS WORDS, or the average word per sentence ratio might give additional information on the tone or sentiment of written text, depending on the context and model.\n",
    "\n",
    "Here are a few basic string methods that can come in handy for those scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there!'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi there!\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello there!'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(\"Hi\",\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there ! '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(\"!\",\" ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi thr!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(\"e\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there!']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi there!'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The few examples above are just to inspire you some ideas. There are many things you could think of to analyze and extract informative summaries from text data. The pandas `<pd.Series>.apply()` method can come in very handy with custom user-defined functions.\n",
    "\n",
    "pd.Series also has a `str` subset of methods for text data. Here are a few dummy examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>my_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi there!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My dog is cute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i lost my wallet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            my_text\n",
       "0         Hi there!\n",
       "1   My dog is cute.\n",
       "2  i lost my wallet"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "str_text = pd.DataFrame({\"my_text\":[\"Hi there!\",\"My dog is cute.\",\"i lost my wallet\"]})\n",
    "str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           Hi there!\n",
       "1     My dog is cute.\n",
       "2    I lost my wallet\n",
       "Name: my_text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text.my_text.str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           hi there!\n",
       "1     my dog is cute.\n",
       "2    i lost my wallet\n",
       "Name: my_text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text.my_text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2     True\n",
       "Name: my_text, dtype: bool"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text.my_text.str.contains(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2     True\n",
       "Name: my_text, dtype: bool"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text.my_text.str.contains(\"my\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "2    1\n",
       "Name: my_text, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text.my_text.str.count(\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Hi thr!\n",
       "1     My dog is cut.\n",
       "2    i lost my wallt\n",
       "Name: my_text, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text.my_text.str.replace(\"e\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more examples in the pandas documentation.\n",
    "\n",
    "For more complicated text processing procedures, one would usually turn to [**regular expressions**](https://en.wikipedia.org/wiki/Regular_expression), as a much more powerful tool. The [`re` module](https://docs.python.org/3/library/re.html) provides the base tools to work with regular expressions in python. Some `pandas`'s  `Series.str` methods above also accept regular expressions.\n",
    "\n",
    "This goes beyond the scope of this seminar, but if you are interested:\n",
    "- [Interactive RegEx tutorials](https://regexr.com/)\n",
    "- [Another tutorial](https://www.w3schools.com/python/python_regex.asp)\n",
    "- And many more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Luis']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re_str = \"Hello, how are you Luis?\"\n",
    "re.findall(\"[A-Z]\\w+\", re_str)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCtguPGtWiKfucgbeYKTic",
   "collapsed_sections": [
    "yv7hitczyvUA",
    "VfzsMmdIymLR",
    "IJnX396HzwRM",
    "7byMSxD0yq0O",
    "TfUp_mj32YHg",
    "oNIGZbIM32VC",
    "ss1gb3fi9svt"
   ],
   "name": "Lab1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
